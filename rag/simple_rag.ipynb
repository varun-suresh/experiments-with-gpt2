{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Build a simple Retrieval-Augmented Generation pipeline to demonstrate its working.\n",
    "\n",
    "Steps:\n",
    "1. Document Store: Use in-memory key-value store.\n",
    "2. Retrieval: Use embeddings from GPT-2\n",
    "3. Generation: Use GPT-2 for generating a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from bert import BERT\n",
    "from bert_config import BERTConfig\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# text_1 = \"Hello,world\"\n",
    "# text_2 = \"Second sentence\"\n",
    "# encoded_input = tokenizer(text_1,text_2,return_tensors=\"pt\")\n",
    "# print(encoded_input)\n",
    "# output_hf = model_hf(**encoded_input)\n",
    "# embedding_output = model_hf.embeddings(encoded_input[\"input_ids\"])\n",
    "# encoder_output = model_hf.encoder(embedding_output)\n",
    "# # print(encoder_output)\n",
    "# print(output_hf)\n",
    "# print(output_hf.last_hidden_state[:,0,:])\n",
    "\n",
    "# model = BERT.from_pretrained(config=BERTConfig())\n",
    "# output = model(**encoded_input)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42259/42259 [22:46<00:00, 30.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Store the embeddings for the dev set\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from transformers import DataCollatorWithPadding\n",
    "from bert import BERT\n",
    "from bert_config import BERTConfig\n",
    "from rag.snliDataset import snliDataset, snliEmbeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = BERT.from_pretrained(config=BERTConfig())\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# model_hf = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# model_hf.to(device)\n",
    "# model_hf.eval()\n",
    "def dynamic_padding(data,device=\"cuda\"):\n",
    "    s1 = [item[\"sentence1\"] for item in data]\n",
    "    s2 = [item[\"sentence2\"] for item in data]\n",
    "    labels = [item[\"label\"] for item in data]\n",
    "    encoded = tokenizer(s1,s2,padding=True,truncation=True,return_tensors=\"pt\",max_length=512)\n",
    "    encoded[\"attention_mask\"] = encoded[\"attention_mask\"].bool()\n",
    "    encoded[\"sentence1\"] = s1\n",
    "    encoded[\"sentece2\"] = s2\n",
    "    encoded = {key: tensor.to(device) for key, tensor in encoded.items()}\n",
    "    return encoded,labels\n",
    "\n",
    "def prepare_data(output_filename:str):\n",
    "    \"\"\"\n",
    "    Store the training data in the following as a json file. Format:\n",
    "    {\n",
    "        'sentence1': [ids for sentence1],\n",
    "        'sentence2': [ids for sentence2],\n",
    "        'embedding': [output of bert for the sentence1, sentence2 input],\n",
    "        'label': int (0,1,2)\n",
    "    }\n",
    "    \"\"\"\n",
    "    split = \"dev\"\n",
    "    sd = snliDataset(split)\n",
    "    batch_size = 64\n",
    "    dl = DataLoader(sd,batch_size=batch_size,collate_fn=dynamic_padding)    \n",
    "\n",
    "    data = []\n",
    "    with torch.no_grad():\n",
    "        for encoded,labels in tqdm(dl):\n",
    "            \n",
    "            output = model(**encoded)\n",
    "            output = output.cpu()\n",
    "            embedding = torch.mean(output,dim=1)\n",
    "            for i in range(batch_size):\n",
    "                data_item = {\n",
    "                    \"sentence1\": encoded[i][\"sentence1\"].cpu(),\n",
    "                    \"sentece2\": encoded[i][\"sentence2\"].cpu(),\n",
    "                    \"embedding\": embedding[i]\n",
    "                    \"label\": labels[i]\n",
    "                }\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    torch.save(embeddings, f\"embeddings_{split}.pt\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619977/3556621109.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  t = torch.load(\"embeddings_train.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([549367, 768])\n"
     ]
    }
   ],
   "source": [
    "t = torch.load(\"embeddings_train.pt\")\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a MLP classifier\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size,hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size,output_size)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(F.relu(self.hidden_layer(x)))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from snliDataset import snliDataset, snliEmbeddings\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "split = \"train\"\n",
    "se_train = snliEmbeddings(split=split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619977/989154548.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(outputs,torch.tensor(batch[\"label\"]).to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8239\n",
      "Epoch [2/50], Loss: 0.7628\n",
      "Epoch [3/50], Loss: 0.7382\n",
      "Epoch [4/50], Loss: 0.7217\n",
      "Epoch [5/50], Loss: 0.7093\n",
      "Epoch [6/50], Loss: 0.6993\n",
      "Epoch [7/50], Loss: 0.6911\n",
      "Epoch [8/50], Loss: 0.6841\n",
      "Epoch [9/50], Loss: 0.6781\n",
      "Epoch [10/50], Loss: 0.6728\n",
      "Epoch [11/50], Loss: 0.6682\n",
      "Epoch [12/50], Loss: 0.6641\n",
      "Epoch [13/50], Loss: 0.6603\n",
      "Epoch [14/50], Loss: 0.6569\n",
      "Epoch [15/50], Loss: 0.6538\n",
      "Epoch [16/50], Loss: 0.6508\n",
      "Epoch [17/50], Loss: 0.6480\n",
      "Epoch [18/50], Loss: 0.6455\n",
      "Epoch [19/50], Loss: 0.6431\n",
      "Epoch [20/50], Loss: 0.6408\n",
      "Epoch [21/50], Loss: 0.6387\n",
      "Epoch [22/50], Loss: 0.6367\n",
      "Epoch [23/50], Loss: 0.6346\n",
      "Epoch [24/50], Loss: 0.6327\n",
      "Epoch [25/50], Loss: 0.6310\n",
      "Epoch [26/50], Loss: 0.6292\n",
      "Epoch [27/50], Loss: 0.6276\n",
      "Epoch [28/50], Loss: 0.6261\n",
      "Epoch [29/50], Loss: 0.6246\n",
      "Epoch [30/50], Loss: 0.6232\n",
      "Epoch [31/50], Loss: 0.6218\n",
      "Epoch [32/50], Loss: 0.6205\n",
      "Epoch [33/50], Loss: 0.6193\n",
      "Epoch [34/50], Loss: 0.6181\n",
      "Epoch [35/50], Loss: 0.6168\n",
      "Epoch [36/50], Loss: 0.6157\n",
      "Epoch [37/50], Loss: 0.6146\n",
      "Epoch [38/50], Loss: 0.6135\n",
      "Epoch [39/50], Loss: 0.6125\n",
      "Epoch [40/50], Loss: 0.6114\n",
      "Epoch [41/50], Loss: 0.6104\n",
      "Epoch [42/50], Loss: 0.6094\n",
      "Epoch [43/50], Loss: 0.6085\n",
      "Epoch [44/50], Loss: 0.6076\n",
      "Epoch [45/50], Loss: 0.6067\n",
      "Epoch [46/50], Loss: 0.6058\n",
      "Epoch [47/50], Loss: 0.6050\n",
      "Epoch [48/50], Loss: 0.6041\n",
      "Epoch [49/50], Loss: 0.6034\n",
      "Epoch [50/50], Loss: 0.6026\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "mlp = MLP(768,100,3)\n",
    "mlp.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(mlp.parameters(),lr=learning_rate)\n",
    "n_epochs = 50\n",
    "train_loader = DataLoader(se_train,batch_size=13)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    mlp.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(batch[\"embedding\"].to(device))\n",
    "        loss = criterion(outputs,torch.tensor(batch[\"label\"]).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 71.78%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "mlp.eval()\n",
    "correct, total = 0, 0\n",
    "se_test = snliEmbeddings(split=\"test\")\n",
    "test_loader = DataLoader(se_test,batch_size=32)\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = mlp(batch[\"embedding\"].to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # print(predicted, batch[\"label\"])\n",
    "        total += batch[\"label\"].size(0)\n",
    "        correct += (predicted == batch[\"label\"].to(device)).sum().item()\n",
    "    \n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
