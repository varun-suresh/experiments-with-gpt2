# Experiments with GPT-2

In this repo, I want to experiment with GPT-2 (124M parameter) model and understand how to train and fine tune it well. Instead of using the [Hugging Face](https://huggingface.co/) implementation, I followed [Andrej Karpathy's nanoGPT implementation](https://github.com/karpathy/nanoGPT/tree/master) and made changes wherever necessary.

As a first experiment, I fine tuned the model for sentiment classification. Results and code are in the sentiment classification folder

