{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Build a simple Retrieval-Augmented Generation pipeline to demonstrate its working.\n",
    "\n",
    "Steps:\n",
    "1. Use Sentence BERT to create embeddings\n",
    "2. Document Store: Use in-memory key-value store.\n",
    "3. Retrieval: Use embeddings from GPT-2\n",
    "4. Generation: Use GPT-2 for generating a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the embeddings for the dev set\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from bert import BERT\n",
    "from bert_config import BERTConfig\n",
    "from rag.snliDataset import snliDataset, snliEmbeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = BERT.from_pretrained(config=BERTConfig())\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def dynamic_padding(data,device=\"cuda\"):\n",
    "    s1 = [item[\"sentence1\"] for item in data]\n",
    "    s2 = [item[\"sentence2\"] for item in data]\n",
    "    labels = [item[\"label\"] for item in data]\n",
    "    encoded = tokenizer(s1,s2,padding=True,truncation=True,return_tensors=\"pt\",max_length=512)\n",
    "    encoded[\"attention_mask\"] = encoded[\"attention_mask\"].bool()\n",
    "    encoded = {key: tensor.to(device) for key, tensor in encoded.items()}\n",
    "    return encoded,labels\n",
    "\n",
    "def prepare_data(split: str, output_filename:str):\n",
    "    \"\"\"\n",
    "    Store the training data in the following as a json file. Format:\n",
    "    {\n",
    "        'input_ids': [tokenized input ids for sentence 1, sentence2]\n",
    "        'embedding': [output of bert for the sentence1, sentence2 input],\n",
    "        'label': int (0,1,2)\n",
    "    }\n",
    "    \"\"\"\n",
    "    sd = snliDataset(split)\n",
    "    batch_size = 64\n",
    "    dl = DataLoader(sd,batch_size=batch_size,collate_fn=dynamic_padding)    \n",
    "\n",
    "    data = []\n",
    "    with torch.no_grad():\n",
    "        for encoded,labels in tqdm(dl):\n",
    "            \n",
    "            output = model(**encoded)\n",
    "            output = output.cpu()\n",
    "            embedding = output\n",
    "            seq_lens = torch.sum(encoded[\"attention_mask\"],dim=1)\n",
    "            for i in range(len(encoded)):\n",
    "                \n",
    "                data_item = {\n",
    "                    \"input_ids\": encoded[\"input_ids\"][i][:seq_lens[i]].cpu(),\n",
    "                    \"embedding\": embedding[i],\n",
    "                    \"label\": labels[i],\n",
    "                }\n",
    "                data.append(data_item)\n",
    "\n",
    "    torch.save({\"data\": data}, output_filename)\n",
    "    \n",
    "    print(f\"Wrote {split} data to {output_filename}\")\n",
    "\n",
    "# prepare_data(\"dev\", \"dev_data.pt\")\n",
    "# prepare_data(\"test\", \"test_data.pt\")\n",
    "prepare_data(\"train\",\"train_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a MLP classifier\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size,hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size,output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(F.relu(self.hidden_layer(x)))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from snliDataset import snliEmbeddings\n",
    "\n",
    "split = \"train\"\n",
    "se_train = snliEmbeddings(split=split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "mlp = MLP(768,256,3)\n",
    "mlp.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(mlp.parameters(),lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=100,gamma=0.5)\n",
    "n_epochs = 500\n",
    "train_loader = DataLoader(se_train,batch_size=64,shuffle=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    mlp.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(batch[\"embedding\"].to(device))\n",
    "        loss = criterion(outputs,torch.tensor(batch[\"label\"]).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()        \n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "mlp.eval()\n",
    "correct, total = 0, 0\n",
    "se_test = snliEmbeddings(split=\"test\")\n",
    "test_loader = DataLoader(se_test,batch_size=32)\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = mlp(batch[\"embedding\"].to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # print(predicted, batch[\"label\"])\n",
    "        total += batch[\"label\"].size(0)\n",
    "        correct += (predicted == batch[\"label\"].to(device)).sum().item()\n",
    "    \n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence BERT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from rag.train import Trainer\n",
    "from rag.snliDataset import sentenceBERTDataset\n",
    "from bert_config import BERTConfig, BERTTrainConfig\n",
    "\n",
    "train_set = sentenceBERTDataset(\"train\")\n",
    "val_set = sentenceBERTDataset(\"dev\")\n",
    "train_config = BERTTrainConfig()\n",
    "model_config = BERTConfig()\n",
    "\n",
    "trainer = Trainer(train_set, val_set,model_config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "import torch\n",
    "from rag.snliDataset import sentenceBERTDataset\n",
    "from bert_config import BERTConfig, BERTTrainConfig\n",
    "\n",
    "from sentenceBERT import sentenceBERT\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from bert_utils import dynamic_padding\n",
    "\n",
    "device = \"cuda\"\n",
    "test_set = sentenceBERTDataset(\"test\")\n",
    "ckpt_path = \"out/bert_ckpt_train.pt\"\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model = sentenceBERT(BERTConfig())\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(device)\n",
    "\n",
    "correct, total = 0, 0\n",
    "test_loader = DataLoader(test_set,batch_size=8,collate_fn=dynamic_padding)\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = model(batch[\"sentence1\"],batch[\"sentence2\"])\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch[\"label\"].size(0)\n",
    "        correct += (predicted == batch[\"label\"].to(device)).sum().item()\n",
    "    \n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Knowledge Base\n",
    "\n",
    "To build out a simple RAG, use a few text documents as the knowledge base.\n",
    "Run sentence BERT on these documents, on overlapping blocks of text and store the vectors.\n",
    "Accept a query from the user\n",
    "Run sentence BERT on the query, retrieve the 3 most relevant embeddings from the knowledge base using cosine distance\n",
    "Provide the user query, retrieved embeddings to GPT-2 and generate a response.\n",
    "\n",
    "\n",
    "For the knowledge base, I copied the text about Tour de France from Wikipedia and saved it in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12775 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Run SBERT on the text document and save the embeddings\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "fpath = \"tdf.txt\"\n",
    "text = open(fpath).read()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "embedding_size = 256\n",
    "encoded = tokenizer(text,return_tensors=\"pt\")\n",
    "enc_size = len(encoded.input_ids[0])\n",
    "input_ids_1 = torch.zeros(enc_size // embedding_size + 1,embedding_size)\n",
    "\n",
    "\n",
    "zeros = torch.zeros((enc_size//embedding_size+1)*embedding_size-enc_size)\n",
    "# print(len(zeros))\n",
    "# print(enc_size)\n",
    "\n",
    "input_ids_1 = torch.reshape(torch.cat((zeros,encoded.input_ids[0])), (enc_size//embedding_size+1, embedding_size))\n",
    "attention_1 = torch.reshape(torch.cat((zeros,encoded.attention_mask[0])), (enc_size//embedding_size+1, embedding_size))\n",
    "token_1 = torch.reshape(torch.cat((zeros,encoded.token_type_ids[0])), (enc_size//embedding_size+1, embedding_size))\n",
    "\n",
    "input_ids_2 = torch.reshape(torch.cat((encoded.input_ids[0],zeros)), (enc_size//embedding_size+1, embedding_size))\n",
    "attention_2 = torch.reshape(torch.cat((encoded.attention_mask[0],zeros)), (enc_size//embedding_size+1, embedding_size))\n",
    "token_2 = torch.reshape(torch.cat((encoded.token_type_ids[0],zeros)), (enc_size//embedding_size+1, embedding_size))\n",
    "\n",
    "input_ids = torch.hstack((input_ids_1,input_ids_2))\n",
    "attention_mask = torch.hstack((attention_1,attention_2))\n",
    "token_type_ids = torch.hstack((token_1,token_2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538404/2226280181.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12775 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentenceBERT import sentenceBERT\n",
    "from bert_config import BERTConfig\n",
    "\n",
    "device = \"cuda\"\n",
    "ckpt_path = \"out/bert_ckpt_train.pt\"\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model = sentenceBERT(BERTConfig())\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(device)\n",
    "\n",
    "fpath = \"/home/varun/projects/experiments-with-gpt2/rag/tdf.txt\"\n",
    "model.encode(fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "encode_output = np.load(\"doc_embeddings.npz\")\n",
    "embeddings = encode_output[\"embeddings\"]\n",
    "input_ids = encode_output[\"input_ids\"]\n",
    "attention_mask = encode_output[\"attention_mask\"]\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "# faiss.normalize_L2(embeddings)\n",
    "index.add(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "[[503.33313 523.7925 ]] [[24  8]]\n",
      "130 ] then the germans invaded and the race was not held again until 1947 ( see tour de france during the second world war ). the first german team after the war was in 1960, although individual germans had ridden in mixed teams. the tour has since started in germany four times : in cologne in 1965, in frankfurt in 1980, in west berlin on the city's 750th anniversary in 1987, and in dusseldorf in 2017. plans to enter east germany in 1987 were abandoned. corsica prior to 2013, the tour de france had visited every region of metropolitan france except corsica. [ 131 ] jean - marie leblanc, when he was organiser, said the island had never asked for a stage start there. it would be difficult to find accommodation for 4, 000 people, he said. [ 132 ] the spokesman of the corsican nationalist party party of the corsican nation, francois alfonsi, said : \" the organisers must be afraid of terrorist attacks. if they are really thinking of a possible terrorist action, they are wrong. our movement, which is nationalist and in favour of self - government, would be delighted if the tour came to corsica. \" [ 132 ] the opening three stages of the 2013 tour de france were held on corsica as part of the celebrations for the 100th edition of the race. start and finish of the tour main article : list of tour de france grands departs most stages are in mainland france, although since the mid - 1950s it has become common to visit nearby countries. [ 133 ] the tour has visited thirteen different countries in its history : andorra, belgium, denmark, germany, ireland, italy, luxembourg, monaco, the netherlands, san marino, spain, switzerland, and the united kingdom, all of which have hosted stages or part of a stage. [ 134 ] since 1975 the finish has been on the champs - elysees in paris ; from 1903 to 1967 the race finished at the parc des princes stadium in western paris and from 1968 to 1974 at the piste municipale south of the capital. [ 100 ] in the 111th edition, because of the 2024 summer olympics in paris, the race ended outside paris for the first time, on the place massena in nice. [ 135 ] felix levitan, race organizer in the 1980s, was keen to host stages in the united states, but these proposals have never been developed. [ 136 ] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "chavanel, among others. bernard hinault at the 1978 tour de france from the late 1970s and into the early 1980s, the tour was dominated by frenchman bernard hinault, who would become the third rider to win five times. hinault was defeated by joop zoetemelk in 1980 when he withdrew, and only once in his tour de france career was he soundly defeated, and this was by laurent fignon in 1984. in 1986, hinault, who had won the year before with american rider greg lemond supporting him, publicly pledged to ride in support of lemond. several attacks during the race cast doubt on the sincerity of his promise, leading to a rift between the two riders and the entire la vie claire team, before lemond prevailed. it was the first ever victory for a rider from outside of europe. the 1986 tour is widely considered to be one of the most memorable in the history of the sport due to the battle between lemond and hinault. the 1987 edition was more uncertain than past editions, as previous winners hinault and zoetemelk had retired, lemond was absent, and fignon was suffering from a lingering injury. as such, the race was highly competitive, and the lead changed hands eight times before stephen roche won. when roche won the world championship road race later in the season, he became only the second rider ( after merckx ) to win cycling's triple crown, which meant winning the giro d'italia, the tour and the road world cycling championship in one calendar year. levitan helped drive an internationalization of the tour de france, and cycling in general. [ 58 ] roche was the first winner from ireland ; however, in the years leading up to his victory, cyclists from numerous other countries began joining the ranks of the peloton. in 1982, sean kelly of ireland ( points ) and phil anderson of australia ( young rider ) became the first winners of any tour classifications from outside cycling's continental europe heartlands, while levitan was influential in facilitating the participation in the 1983 tour by amateur riders from the eastern bloc and colombia. [ 58 ] in 1984, for the first time, the societe du tour de france organized the tour de france feminin, a version for women. [ n 5 ] it was run in the same weeks as the men's version, and it was won by marianne martin. [ 60 ] while the global awareness and popularity of the tour grew during this time, its finances became stretched\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "query = \"Name the three grand tours?\"\n",
    "input = tokenizer(query,return_tensors=\"pt\")\n",
    "input[\"attention_mask\"] = input[\"attention_mask\"].bool()\n",
    "input= {key: tensor.to(device) for key, tensor in input.items()}\n",
    "print(input[\"input_ids\"].size())\n",
    "with torch.no_grad():\n",
    "    q_embedding = model.bert(**input).cpu().numpy()\n",
    "dist, ann = index.search(q_embedding,2)\n",
    "\n",
    "print(dist, ann)\n",
    "for n in ann[0]:\n",
    "    print(tokenizer.decode(input_ids[n,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153901272062508\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "print(distance.cosine(q_embedding[0],embeddings[8,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the documents using the embedding_model - i.e For each sentence (of sentence_size) in the document, create an embedding. Return the embeddings\n",
    "class RAG:\n",
    "  def create_knowledge_base(embedding_model, text_docs,sentence_size,overlap_size):\n",
    "    output_embeddings = []\n",
    "    output_text = []\n",
    "    for doc in text_docs:\n",
    "        doc_embeddings, doc_text = embedding_model.encode(open(doc).read(),\n",
    "                                                          sentence_size,\n",
    "                                                          overlap_size)\n",
    "        output_embeddings.append(doc_embeddings)\n",
    "        output_text.extend(doc_text)\n",
    "\n",
    "    return np.vstack(output_embeddings), output_text\n",
    "\n",
    "\n",
    "  # Create an index with all the embeddings\n",
    "  def create_index(embeddings):\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "  # Get the top k documents most relevant to query\n",
    "  def retrieve(index,embedding_model,query,k):\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    dist, ann = index.search(query_embedding,k)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
