{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Build a simple Retrieval-Augmented Generation pipeline to demonstrate its working.\n",
    "\n",
    "Steps:\n",
    "1. Use Sentence BERT to create embeddings\n",
    "2. Document Store: Use in-memory key-value store.\n",
    "3. Retrieval: Use embeddings from GPT-2\n",
    "4. Generation: Use GPT-2 for generating a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the embeddings for the dev set\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from bert import BERT\n",
    "from bert_config import BERTConfig\n",
    "from rag.snliDataset import snliDataset, snliEmbeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = BERT.from_pretrained(config=BERTConfig())\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def dynamic_padding(data,device=\"cuda\"):\n",
    "    s1 = [item[\"sentence1\"] for item in data]\n",
    "    s2 = [item[\"sentence2\"] for item in data]\n",
    "    labels = [item[\"label\"] for item in data]\n",
    "    encoded = tokenizer(s1,s2,padding=True,truncation=True,return_tensors=\"pt\",max_length=512)\n",
    "    encoded[\"attention_mask\"] = encoded[\"attention_mask\"].bool()\n",
    "    encoded = {key: tensor.to(device) for key, tensor in encoded.items()}\n",
    "    return encoded,labels\n",
    "\n",
    "def prepare_data(split: str, output_filename:str):\n",
    "    \"\"\"\n",
    "    Store the training data in the following as a json file. Format:\n",
    "    {\n",
    "        'input_ids': [tokenized input ids for sentence 1, sentence2]\n",
    "        'embedding': [output of bert for the sentence1, sentence2 input],\n",
    "        'label': int (0,1,2)\n",
    "    }\n",
    "    \"\"\"\n",
    "    sd = snliDataset(split)\n",
    "    batch_size = 64\n",
    "    dl = DataLoader(sd,batch_size=batch_size,collate_fn=dynamic_padding)    \n",
    "\n",
    "    data = []\n",
    "    with torch.no_grad():\n",
    "        for encoded,labels in tqdm(dl):\n",
    "            \n",
    "            output = model(**encoded)\n",
    "            output = output.cpu()\n",
    "            embedding = output\n",
    "            seq_lens = torch.sum(encoded[\"attention_mask\"],dim=1)\n",
    "            for i in range(len(encoded)):\n",
    "                \n",
    "                data_item = {\n",
    "                    \"input_ids\": encoded[\"input_ids\"][i][:seq_lens[i]].cpu(),\n",
    "                    \"embedding\": embedding[i],\n",
    "                    \"label\": labels[i],\n",
    "                }\n",
    "                data.append(data_item)\n",
    "\n",
    "    torch.save({\"data\": data}, output_filename)\n",
    "    \n",
    "    print(f\"Wrote {split} data to {output_filename}\")\n",
    "\n",
    "# prepare_data(\"dev\", \"dev_data.pt\")\n",
    "# prepare_data(\"test\", \"test_data.pt\")\n",
    "prepare_data(\"train\",\"train_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a MLP classifier\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size,hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size,output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(F.relu(self.hidden_layer(x)))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from snliDataset import snliEmbeddings\n",
    "\n",
    "split = \"train\"\n",
    "se_train = snliEmbeddings(split=split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "mlp = MLP(768,256,3)\n",
    "mlp.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(mlp.parameters(),lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=100,gamma=0.5)\n",
    "n_epochs = 500\n",
    "train_loader = DataLoader(se_train,batch_size=64,shuffle=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    mlp.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(batch[\"embedding\"].to(device))\n",
    "        loss = criterion(outputs,torch.tensor(batch[\"label\"]).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()        \n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "mlp.eval()\n",
    "correct, total = 0, 0\n",
    "se_test = snliEmbeddings(split=\"test\")\n",
    "test_loader = DataLoader(se_test,batch_size=32)\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = mlp(batch[\"embedding\"].to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # print(predicted, batch[\"label\"])\n",
    "        total += batch[\"label\"].size(0)\n",
    "        correct += (predicted == batch[\"label\"].to(device)).sum().item()\n",
    "    \n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence BERT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from rag.train import Trainer\n",
    "from rag.snliDataset import sentenceBERTDataset\n",
    "from bert_config import BERTConfig, BERTTrainConfig\n",
    "\n",
    "train_set = sentenceBERTDataset(\"train\")\n",
    "val_set = sentenceBERTDataset(\"dev\")\n",
    "train_config = BERTTrainConfig()\n",
    "model_config = BERTConfig()\n",
    "\n",
    "trainer = Trainer(train_set, val_set,model_config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "import torch\n",
    "from rag.snliDataset import sentenceBERTDataset\n",
    "from bert_config import BERTConfig, BERTTrainConfig\n",
    "\n",
    "from sentenceBERT import sentenceBERT\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from bert_utils import dynamic_padding\n",
    "\n",
    "device = \"cuda\"\n",
    "test_set = sentenceBERTDataset(\"test\")\n",
    "ckpt_path = \"out/bert_ckpt_train.pt\"\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model = sentenceBERT(BERTConfig())\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(device)\n",
    "\n",
    "correct, total = 0, 0\n",
    "test_loader = DataLoader(test_set,batch_size=8,collate_fn=dynamic_padding)\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = model(batch[\"sentence1\"],batch[\"sentence2\"])\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch[\"label\"].size(0)\n",
    "        correct += (predicted == batch[\"label\"].to(device)).sum().item()\n",
    "    \n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Knowledge Base\n",
    "\n",
    "To build out a simple RAG, use a few text documents as the knowledge base.\n",
    "Run sentence BERT on these documents, on overlapping blocks of text and store the vectors.\n",
    "Accept a query from the user\n",
    "Run sentence BERT on the query, retrieve the 3 most relevant embeddings from the knowledge base using cosine distance\n",
    "Provide the user query, retrieved embeddings to GPT-2 and generate a response.\n",
    "\n",
    "\n",
    "For the knowledge base, I copied the text about Tour de France from Wikipedia and saved it in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12775 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Run SBERT on the text document and save the embeddings\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "fpath = \"tdf.txt\"\n",
    "text = open(fpath).read()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "embedding_size = 256\n",
    "encoded = tokenizer(text,return_tensors=\"pt\")\n",
    "enc_size = len(encoded.input_ids[0])\n",
    "input_ids_1 = torch.zeros(enc_size // embedding_size + 1,embedding_size)\n",
    "\n",
    "\n",
    "zeros = torch.zeros((enc_size//embedding_size+1)*embedding_size-enc_size)\n",
    "# print(len(zeros))\n",
    "# print(enc_size)\n",
    "\n",
    "input_ids_1 = torch.reshape(torch.cat((zeros,encoded.input_ids[0])), (enc_size//embedding_size+1, embedding_size))\n",
    "attention_1 = torch.reshape(torch.cat((zeros,encoded.attention_mask[0])), (enc_size//embedding_size+1, embedding_size))\n",
    "token_1 = torch.reshape(torch.cat((zeros,encoded.token_type_ids[0])), (enc_size//embedding_size+1, embedding_size))\n",
    "\n",
    "input_ids_2 = torch.reshape(torch.cat((encoded.input_ids[0],zeros)), (enc_size//embedding_size+1, embedding_size))\n",
    "attention_2 = torch.reshape(torch.cat((encoded.attention_mask[0],zeros)), (enc_size//embedding_size+1, embedding_size))\n",
    "token_2 = torch.reshape(torch.cat((encoded.token_type_ids[0],zeros)), (enc_size//embedding_size+1, embedding_size))\n",
    "\n",
    "input_ids = torch.hstack((input_ids_1,input_ids_2))\n",
    "attention_mask = torch.hstack((attention_1,attention_2))\n",
    "token_type_ids = torch.hstack((token_1,token_2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1538404/2226280181.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12775 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentenceBERT import sentenceBERT\n",
    "from bert_config import BERTConfig\n",
    "\n",
    "device = \"cuda\"\n",
    "ckpt_path = \"out/bert_ckpt_train.pt\"\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model = sentenceBERT(BERTConfig())\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(device)\n",
    "\n",
    "fpath = \"/home/varun/projects/experiments-with-gpt2/rag/tdf.txt\"\n",
    "model.encode(fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "encode_output = np.load(\"doc_embeddings.npz\")\n",
    "embeddings = encode_output[\"embeddings\"]\n",
    "input_ids = encode_output[\"input_ids\"]\n",
    "attention_mask = encode_output[\"attention_mask\"]\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "# faiss.normalize_L2(embeddings)\n",
    "index.add(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m tokenizer(query,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mbool()\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m {key: tensor\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m key, tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "query = \"Name the three grand tours?\"\n",
    "input = tokenizer(query,return_tensors=\"pt\")\n",
    "input[\"attention_mask\"] = input[\"attention_mask\"].bool()\n",
    "input= {key: tensor.to(device) for key, tensor in input.items()}\n",
    "print(input[\"input_ids\"].size())\n",
    "with torch.no_grad():\n",
    "    q_embedding = model.bert(**input).cpu().numpy()\n",
    "dist, ann = index.search(q_embedding,2)\n",
    "\n",
    "print(dist, ann)\n",
    "for n in ann[0]:\n",
    "    print(tokenizer.decode(input_ids[n,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1669026/3587545882.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for bert\n",
      "Loading pre-trained weights for gpt2\n",
      "Number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "from rag import RAG\n",
    "import torch\n",
    "from sentenceBERT import sentenceBERT\n",
    "from bert_config import BERTConfig\n",
    "from gpt import GPT\n",
    "from gpt_config import GPTConfig\n",
    "\n",
    "device = \"cuda\"\n",
    "ckpt_path = \"out/bert_ckpt_train.pt\"\n",
    "ckpt = torch.load(ckpt_path)\n",
    "embedding_model_config = BERTConfig()\n",
    "embedding_model = sentenceBERT(embedding_model_config)\n",
    "embedding_model.to(device)\n",
    "\n",
    "generate_model = GPT.from_pretrained(GPTConfig(block_size=1024))\n",
    "generate_model.to(device)\n",
    "embedding_model_size = embedding_model_config.embedding_size\n",
    "sentence_size = 3\n",
    "overlap_size = 1\n",
    "k = 5\n",
    "\n",
    "rag = RAG(embedding_model,embedding_model_size,generate_model,sentence_size,overlap_size,k)\n",
    "rag.add_to_knowledge_base([\"/home/varun/projects/experiments-with-gpt2/rag/tdf.txt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who won the tour in 2024?, Response:  20:59: Kim Racis has won the\n"
     ]
    }
   ],
   "source": [
    "query = \"Who won the tour in 2024?\"\n",
    "response = rag.get_response(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for gpt2\n",
      "Number of parameters: 123.65M\n",
      "A long time ago a lot of things, people are going to \"Well. I want to\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "from gpt import GPT\n",
    "from gpt_config import GPTConfig\n",
    "import torch\n",
    "torch.manual_seed(23)\n",
    "device=\"cuda\"\n",
    "model = GPT.from_pretrained(GPTConfig(block_size=1024))\n",
    "model.to(device)\n",
    "\n",
    "print(model.generate(\"A long time ago\", max_new_tokens=15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
