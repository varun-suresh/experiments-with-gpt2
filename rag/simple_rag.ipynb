{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Build a simple Retrieval-Augmented Generation pipeline to demonstrate its working.\n",
    "\n",
    "Steps:\n",
    "1. Document Store: Use in-memory key-value store.\n",
    "2. Retrieval: Use embeddings from GPT-2\n",
    "3. Generation: Use GPT-2 for generating a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from bert import BERT\n",
    "from bert_config import BERTConfig\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# text_1 = \"Hello,world\"\n",
    "# text_2 = \"Second sentence\"\n",
    "# encoded_input = tokenizer(text_1,text_2,return_tensors=\"pt\")\n",
    "# print(encoded_input)\n",
    "# output_hf = model_hf(**encoded_input)\n",
    "# embedding_output = model_hf.embeddings(encoded_input[\"input_ids\"])\n",
    "# encoder_output = model_hf.encoder(embedding_output)\n",
    "# # print(encoder_output)\n",
    "# print(output_hf)\n",
    "# print(output_hf.last_hidden_state[:,0,:])\n",
    "\n",
    "# model = BERT.from_pretrained(config=BERTConfig())\n",
    "# output = model(**encoded_input)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4921 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ..., False, False, False]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Store the embeddings for the dev set\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from transformers import DataCollatorWithPadding\n",
    "from bert import BERT\n",
    "from bert_config import BERTConfig\n",
    "from rag.snliDataset import snliDataset, snliEmbeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = BERT.from_pretrained(config=BERTConfig())\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model_hf = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model_hf.to(device)\n",
    "model_hf.eval()\n",
    "def dynamic_padding(data,device=\"cuda\"):\n",
    "    s1 = [item[\"sentence1\"] for item in data]\n",
    "    s2 = [item[\"sentence2\"] for item in data]\n",
    "    labels = [item[\"label\"] for item in data]\n",
    "    encoded = tokenizer(s1,s2,padding=True,truncation=True,return_tensors=\"pt\",max_length=512)\n",
    "    encoded[\"attention_mask\"] = encoded[\"attention_mask\"].bool()\n",
    "    encoded = {key: tensor.to(device) for key, tensor in encoded.items()}\n",
    "    return encoded,labels\n",
    "\n",
    "\n",
    "split = \"dev\"\n",
    "sd = snliDataset(split)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "dl = DataLoader(sd,batch_size=batch_size,collate_fn=dynamic_padding)    \n",
    "\n",
    "embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for encoded,labels in tqdm(dl):\n",
    "        \n",
    "        output = model(**encoded)\n",
    "        \n",
    "        output_hf_batch = model_hf(**encoded)\n",
    "        print(output)\n",
    "        print(\"-------------\")\n",
    "        print(torch.mean(output_hf_batch.last_hidden_state,dim=1))\n",
    "        # embedding_output = model_hf.embeddings(encoded[\"input_ids\"])\n",
    "        # print(f\"HF embedding: {embedding_output[0,0,:]}\")\n",
    "        print(torch.isclose(output,torch.mean(output_hf_batch.last_hidden_state,dim=1)))\n",
    "        break\n",
    "        \n",
    "        # output = output_hf_batch.last_hidden_state.cpu()\n",
    "        # embeddings.append(torch.mean(output,dim=1))\n",
    "\n",
    "# embeddings = torch.cat(embeddings)\n",
    "# torch.save(embeddings, f\"embeddings_{split}.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a MLP classifier\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size,hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size,output_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(F.relu(self.hidden_layer(x)))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from snliDataset import snliDataset, snliEmbeddings\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "split = \"dev\"\n",
    "se_train = snliEmbeddings(split=split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "mlp = MLP(768,100,3)\n",
    "mlp.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(mlp.parameters(),lr=learning_rate)\n",
    "n_epochs = 50\n",
    "train_loader = DataLoader(se_train,batch_size=64,shuffle=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    mlp.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(batch[\"embedding\"].to(device))\n",
    "        loss = criterion(outputs,torch.tensor(batch[\"label\"]).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "mlp.eval()\n",
    "correct, total = 0, 0\n",
    "se_test = snliEmbeddings(split=\"test\")\n",
    "test_loader = DataLoader(se_test,batch_size=1)\n",
    "# test_loader = DataLoader(sd,batch_size=32)\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = mlp(batch[\"embedding\"].to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # print(predicted, batch[\"label\"])\n",
    "        total += batch[\"label\"].size(0)\n",
    "        correct += (predicted == batch[\"label\"].to(device)).sum().item()\n",
    "    \n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
