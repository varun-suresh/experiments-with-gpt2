# Experiments with GPT-2
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/language-models-are-unsupervised-multitask/sentiment-analysis-on-imdb)](https://paperswithcode.com/sota/sentiment-analysis-on-imdb?p=language-models-are-unsupervised-multitask)

In this repo, I want to experiment with GPT-2 (124M parameter) model and understand how to train and fine tune it well. Instead of using the [Hugging Face](https://huggingface.co/) implementation, I followed [Andrej Karpathy's nanoGPT implementation](https://github.com/karpathy/nanoGPT/tree/master) and made changes wherever necessary.

As a first experiment, I fine tuned the model for sentiment classification. Results and code are in the sentiment classification folder

