{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 for Sentiment Analysis on IMDb movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](##Introduction)\n",
    "2. [Data exploration](##Data-Exploration)\n",
    "3. [Zero Shot Classification](##Zero-shot-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The [IMDb](https://ai.stanford.edu/~amaas/data/sentiment/) is a binary sentiment classification dataset consisting of 100k movie reviews(50k positive and 50k negative). The dataset is split into train and test containing 50k reviews each.\n",
    "\n",
    "In this notebook, my goals are:\n",
    "1. Understand and implement [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Run GPT-2 on the IMDb classification task.\n",
    "2. Fine-tune GPT-2 for sentiment classification in under ~30 minutes on a 8GB Nvidia 1080 GTX (Faster if you have a better, newer GPU).\n",
    "3. Understand how [LoRA](https://arxiv.org/abs/2106.09685) is implemented and use it to fine-tune GPT-2 for sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Exploration\n",
    "Get a summary of the dataset. i.e\n",
    "1. No of samples\n",
    "2. No of positive / negative samples.\n",
    "3. Length of the movie reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "import pandas\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from gpt_config import GPTConfig\n",
    "from reviewsDataset import reviewsDataset\n",
    "from eval import Eval\n",
    "from eval_config import EvalConfig\n",
    "from train import Trainer\n",
    "from train_config import TrainConfig\n",
    "from gpt_utils import start_recording, stop_recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset exploration\n",
    "\n",
    "imdb_train = reviewsDataset(\"train\",max_length=10000)\n",
    "imdb_test = reviewsDataset(\"test\",max_length=10000)\n",
    "\n",
    "\n",
    "def format_data(dataset: Dataset) -> pandas.DataFrame:\n",
    "\n",
    "    data = []\n",
    "    for batch in dataset:\n",
    "        data.append({\"input_ids\":len(batch[\"input_ids\"]),\n",
    "                    \"label\": batch[\"label\"],\n",
    "                    \"filename\": batch[\"fpath\"]})\n",
    "    \n",
    "    return pandas.DataFrame(data)\n",
    "\n",
    "train_data = format_data(imdb_train)\n",
    "test_data = format_data(imdb_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summary statistics of the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(data: pandas.DataFrame) -> None:\n",
    "    print(f\"Number of reviews: {len(data)}\")\n",
    "    print(f\"Positive Reviews: {data[data['label'] == 1]['label'].count()}\")\n",
    "    print(f\"Negative Reviews: {data[data['label'] == 0]['label'].count()}\")\n",
    "    print(f\"Max Review Length: {data['input_ids'].max()}\\nMin Review Length: {data['input_ids'].min()}\")\n",
    "    print(f\"Median Review Length: {data['input_ids'].median()}\\nMean Review Length: {data['input_ids'].mean()}\")\n",
    "\n",
    "print(\"Train\\n--------------\")\n",
    "summary(train_data)\n",
    "print(\"Test\\n---------------\")\n",
    "summary(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Length of reviews (measured by the number of tokens)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_hist(title: str,df: pandas.DataFrame) -> None:\n",
    "    plt.figure()\n",
    "    plt.hist(df[\"input_ids\"],bins=100)\n",
    "    plt.xlabel(\"No of tokens\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"{title}\")\n",
    "\n",
    "plot_hist(title='Train Data', df=train_data) \n",
    "plot_hist(title=\"Test Data\", df=test_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(title=\"Positive Reviews Test\",df=test_data[test_data['label']==1])\n",
    "plot_hist(title=\"Negative Reviews Test\",df=test_data[test_data['label']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test.py in `sentiment_classification` and write the results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_by_bin(results, bins,threshold=0.5):\n",
    "    TP = len(results[(results[\"label\"] >= threshold) & (results[\"prediction\"] >= threshold)])\n",
    "    FP = len(results[(results[\"label\"] < threshold) & (results[\"prediction\"] >= threshold)])\n",
    "    TN = len(results[(results[\"label\"] < threshold) & (results[\"prediction\"] < threshold)])\n",
    "    FN = len(results[(results[\"label\"] > threshold) & (results[\"prediction\"] < threshold)])\n",
    "    \n",
    "    print(\"Metrics\")\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    print(f\"Precision: {precision}\\nRecall: {recall}\\nAccuracy: {accuracy}\")\n",
    "    results[\"bin\"] = pandas.cut(results['length'],bins)\n",
    "    metrics_by_bin = results.groupby('bin').apply(lambda x: pandas.Series({\"TP\": ((x[\"label\"] >= threshold) & (x[\"prediction\"] >= threshold)).sum(),\n",
    "                                                                            \"FP\":((x[\"label\"] < threshold) & (x[\"prediction\"] >= threshold)).sum(),\n",
    "                                                                            \"FN\": ((x[\"label\"] >= threshold) & (x[\"prediction\"] < threshold)).sum(),\n",
    "                                                                            \"TN\": ((x[\"label\"] < threshold) & (x[\"prediction\"] < threshold)).sum()}))\n",
    "\n",
    "    metrics_by_bin[\"accuracy\"] = (metrics_by_bin[\"TP\"] + metrics_by_bin[\"TN\"])/(metrics_by_bin[\"TP\"] + metrics_by_bin[\"TN\"]+ metrics_by_bin[\"FP\"]+ metrics_by_bin[\"FN\"])\n",
    "    metrics_by_bin[\"precision\"] = metrics_by_bin[\"TP\"]/(metrics_by_bin[\"TP\"] + metrics_by_bin[\"FP\"])\n",
    "    metrics_by_bin[\"recall\"] = metrics_by_bin[\"TP\"]/(metrics_by_bin[\"TP\"] + metrics_by_bin[\"FN\"])\n",
    "    print(\"Metrics by bin\")\n",
    "    print(metrics_by_bin.to_markdown())\n",
    "    return {\"precision\":precision,\"recall\":recall,\"accuracy\":accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot\n",
    "\n",
    "### Predict the next word given the following prompt\n",
    " \n",
    "Review: {review} Sentiment:\n",
    "\n",
    "I calculate the probabilities of the word \" Positive\" and \" Negative\" and classify the review based on which probability is greater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run evaluation for the zero shot approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_recording(\"gpu_memory_log.txt\")\n",
    "model_config = GPTConfig(block_size=256,use_lora=False)\n",
    "eval_config = EvalConfig(results_path=\"zero_shot_256.txt\",subset=True,batch_size=128)\n",
    "test_set = reviewsDataset(split=\"test\",max_length=model_config.block_size)\n",
    "evaluator = Eval(test_set=test_set,eval_config=eval_config,model_config=model_config)\n",
    "evaluator.evaluate()\n",
    "stop_recording()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = pandas.read_csv(\"zero_shot_256.txt\")\n",
    "bins = range(0,1500,256)\n",
    "get_metrics_by_bin(res_file,bins,threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finetuning without LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_recording(\"without_lora.txt\")\n",
    "train_config = TrainConfig(out_dir=\"run/10_layer_freeze/\",init_from=\"gpt2\",checkpoint_name=\"finetune_no_lora.ckpt\")\n",
    "model_config = GPTConfig(use_lora=False,block_size=256,binary_classification_head=True)\n",
    "rd = reviewsDataset(split=\"train\",max_length=model_config.block_size)\n",
    "train_set, val_set = torch.utils.data.random_split(rd,[0.85,0.15])\n",
    "trainer = Trainer(train_set,val_set,train_config,model_config)\n",
    "trainer.train()\n",
    "stop_recording()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run eval using the fine-tuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = GPTConfig(block_size=256,use_lora=False,load_from_checkpoint=True,binary_classification_head=True,checkpoint_path=\"run/10_layer_freeze/finetune_no_lora.ckpt\")\n",
    "eval_config = EvalConfig(results_path=\"finetuned_no_lora.txt\",subset=False)\n",
    "test_set = reviewsDataset(split=\"test\",cache_dir=\"/home/varun/Downloads/aclImdb/\",max_length=256)\n",
    "evaluator = Eval(test_set=test_set,eval_config=eval_config,model_config=model_config)\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the performance of the fine-tuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = pandas.read_csv(\"finetuned_no_lora.txt\")\n",
    "bins = range(0,1500,256)\n",
    "get_metrics_by_bin(res_file,bins,threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run training using LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_recording(\"with_lora.txt\")\n",
    "train_config = TrainConfig(out_dir=\"run/10_layer_freeze_lora\",\n",
    "                            checkpoint_name=\"finetune_lora.ckpt\",\n",
    "                            init_from=\"gpt2\")\n",
    "model_config = GPTConfig(block_size=256,\n",
    "                        use_lora=True,\n",
    "                        r=8,\n",
    "                        lora_layers=(10,11),\n",
    "                        binary_classification_head=True)\n",
    "rd = reviewsDataset(split=\"train\",max_length=model_config.block_size,cache_dir=\"/home/varun/Downloads/aclImdb/\")\n",
    "train_set, val_set = torch.utils.data.random_split(rd,[0.85,0.15])\n",
    "trainer = Trainer(train_set,val_set,train_config,model_config)\n",
    "trainer.train()\n",
    "stop_recording()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate using the LoRA finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_config = GPTConfig(load_from_checkpoint=True,checkpoint_path=\"run/10_layer_freeze_lora/finetune_lora.ckpt\")\n",
    "eval_config = EvalConfig(results_path=\"run/10_layer_freeze_lora/finetuned_lora.txt\",batch_size=8,subset=False)\n",
    "test_set = reviewsDataset(split=\"test\",cache_dir=\"/home/varun/Downloads/aclImdb/\",max_length=model_config.block_size)\n",
    "evaluator = Eval(test_set=test_set,eval_config=eval_config,model_config=model_config)\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = pandas.read_csv(\"run/10_layer_freeze_lora/finetuned_lora.txt\")\n",
    "bins = range(0,1500,256)\n",
    "results = get_metrics_by_bin(res_file,bins,threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "log_file = \"with_lora.txt\"\n",
    "data = pd.read_csv(log_file, names=[\"timestamp\", \"memory\"], sep=\", \")\n",
    "data[\"memory\"] = data[\"memory\"].str.replace(\" MiB\", \"\").astype(int)\n",
    "\n",
    "# Plot memory usage\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(pd.to_datetime(data[\"timestamp\"]), data[\"memory\"], label=\"GPU Memory Usage\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Memory Usage (MB)\")\n",
    "plt.title(\"GPU Memory Usage Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for bert\n",
      "Setting up LoRA for layer 10\n",
      "Setting up LoRA for layer 11\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from train_bert import Trainer\n",
    "from reviewsDataset import reviewsDataset\n",
    "from language_models.bert_config import BERTTrainConfig\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(reviewsDataset(\"train\"),[0.9,0.1])\n",
    "test_set = reviewsDataset(\"test\") \n",
    "config = BERTTrainConfig(model_type=\"sc-bert\",freeze_layers=10,checkpoint_name=\"sc_bert_lora.pt\",eval_size=50,init_from=\"bert\",max_iters=14000)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "trainer = Trainer(config,train_set,val_set,test_set,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.79it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.45it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.71it/s]\n",
      "  0%|          | 1/14000 [00:28<110:18:23, 28.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Train Loss: 0.7058370113372803\n",
      "Validation Loss:0.6977845430374146\n",
      "Train Error: 0.52\n",
      "Validation Error: 0.52\n",
      "Test Error: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1561/14000 [04:15<30:17,  6.85it/s] "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from out/sc_bert.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [06:58<00:00,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10719999999999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from train_bert import Trainer\n",
    "from reviewsDataset import reviewsDataset\n",
    "from language_models.bert_config import BERTTrainConfig\n",
    "\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(reviewsDataset(\"train\"),[0.9,0.1])\n",
    "test_set = reviewsDataset(split=\"test\")\n",
    "config = BERTTrainConfig(model_type=\"sc-bert\",init_from=\"resume\",freeze_layers=10,checkpoint_name=\"sc_bert_lora.pt\",eval_size=50)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "trainer = Trainer(config,train_set,val_set,test_set,criterion)\n",
    "print(trainer.calculate_subset_error(test_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
