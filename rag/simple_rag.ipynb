{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Build a simple Retrieval-Augmented Generation pipeline to demonstrate its working.\n",
    "\n",
    "Steps:\n",
    "1. Document Store: Use in-memory key-value store.\n",
    "2. Retrieval: Use embeddings from GPT-2\n",
    "3. Generation: Use GPT-2 for generating a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for BERT\n"
     ]
    }
   ],
   "source": [
    "# Store the embeddings for the dev set\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "import json\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from transformers import DataCollatorWithPadding\n",
    "from bert import BERT\n",
    "from bert_config import BERTConfig\n",
    "from rag.snliDataset import snliDataset, snliEmbeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = BERT.from_pretrained(config=BERTConfig())\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "device=\"cuda\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def dynamic_padding(data,device=\"cuda\"):\n",
    "    s1 = [item[\"sentence1\"] for item in data]\n",
    "    s2 = [item[\"sentence2\"] for item in data]\n",
    "    labels = [item[\"label\"] for item in data]\n",
    "    encoded = tokenizer(s1,s2,padding=True,truncation=True,return_tensors=\"pt\",max_length=512)\n",
    "    encoded[\"attention_mask\"] = encoded[\"attention_mask\"].bool()\n",
    "    encoded = {key: tensor.to(device) for key, tensor in encoded.items()}\n",
    "    return encoded,labels\n",
    "\n",
    "def prepare_data(split: str, output_filename:str):\n",
    "    \"\"\"\n",
    "    Store the training data in the following as a json file. Format:\n",
    "    {\n",
    "        'input_ids': [tokenized input ids for sentence 1, sentence2]\n",
    "        'embedding': [output of bert for the sentence1, sentence2 input],\n",
    "        'label': int (0,1,2)\n",
    "    }\n",
    "    \"\"\"\n",
    "    sd = snliDataset(split)\n",
    "    batch_size = 64\n",
    "    dl = DataLoader(sd,batch_size=batch_size,collate_fn=dynamic_padding)    \n",
    "\n",
    "    data = []\n",
    "    with torch.no_grad():\n",
    "        for encoded,labels in tqdm(dl):\n",
    "            \n",
    "            output = model(**encoded)\n",
    "            output = output.cpu()\n",
    "            embedding = output\n",
    "            seq_lens = torch.sum(encoded[\"attention_mask\"],dim=1)\n",
    "            for i in range(len(encoded)):\n",
    "                \n",
    "                data_item = {\n",
    "                    \"input_ids\": encoded[\"input_ids\"][i][:seq_lens[i]].cpu(),\n",
    "                    \"embedding\": embedding[i],\n",
    "                    \"label\": labels[i],\n",
    "                }\n",
    "                data.append(data_item)\n",
    "\n",
    "    torch.save({\"data\": data}, output_filename)\n",
    "    \n",
    "    print(f\"Wrote {split} data to {output_filename}\")\n",
    "\n",
    "# prepare_data(\"dev\", \"dev_data.pt\")\n",
    "# prepare_data(\"test\", \"test_data.pt\")\n",
    "prepare_data(\"train\",\"train_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a MLP classifier\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size,hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size,output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(F.relu(self.hidden_layer(x)))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/varun/projects/experiments-with-gpt2/\")\n",
    "\n",
    "from snliDataset import snliDataset, snliEmbeddings\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "split = \"dev\"\n",
    "se_train = snliEmbeddings(split=split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_825837/2867753000.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(outputs,torch.tensor(batch[\"label\"]).to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 1.1131\n",
      "Epoch [2/500], Loss: 1.1030\n",
      "Epoch [3/500], Loss: 1.0953\n",
      "Epoch [4/500], Loss: 1.0983\n",
      "Epoch [5/500], Loss: 1.0935\n",
      "Epoch [6/500], Loss: 1.0955\n",
      "Epoch [7/500], Loss: 1.0884\n",
      "Epoch [8/500], Loss: 1.0844\n",
      "Epoch [9/500], Loss: 1.0917\n",
      "Epoch [10/500], Loss: 1.0839\n",
      "Epoch [11/500], Loss: 1.0950\n",
      "Epoch [12/500], Loss: 1.0894\n",
      "Epoch [13/500], Loss: 1.0937\n",
      "Epoch [14/500], Loss: 1.0766\n",
      "Epoch [15/500], Loss: 1.0751\n",
      "Epoch [16/500], Loss: 1.0789\n",
      "Epoch [17/500], Loss: 1.0756\n",
      "Epoch [18/500], Loss: 1.0881\n",
      "Epoch [19/500], Loss: 1.0775\n",
      "Epoch [20/500], Loss: 1.0838\n",
      "Epoch [21/500], Loss: 1.0759\n",
      "Epoch [22/500], Loss: 1.0717\n",
      "Epoch [23/500], Loss: 1.0759\n",
      "Epoch [24/500], Loss: 1.0701\n",
      "Epoch [25/500], Loss: 1.0690\n",
      "Epoch [26/500], Loss: 1.0701\n",
      "Epoch [27/500], Loss: 1.0654\n",
      "Epoch [28/500], Loss: 1.0708\n",
      "Epoch [29/500], Loss: 1.0670\n",
      "Epoch [30/500], Loss: 1.0652\n",
      "Epoch [31/500], Loss: 1.0644\n",
      "Epoch [32/500], Loss: 1.0679\n",
      "Epoch [33/500], Loss: 1.0710\n",
      "Epoch [34/500], Loss: 1.0672\n",
      "Epoch [35/500], Loss: 1.0641\n",
      "Epoch [36/500], Loss: 1.0652\n",
      "Epoch [37/500], Loss: 1.0743\n",
      "Epoch [38/500], Loss: 1.0519\n",
      "Epoch [39/500], Loss: 1.0611\n",
      "Epoch [40/500], Loss: 1.0638\n",
      "Epoch [41/500], Loss: 1.0631\n",
      "Epoch [42/500], Loss: 1.0659\n",
      "Epoch [43/500], Loss: 1.0661\n",
      "Epoch [44/500], Loss: 1.0557\n",
      "Epoch [45/500], Loss: 1.0637\n",
      "Epoch [46/500], Loss: 1.0574\n",
      "Epoch [47/500], Loss: 1.0548\n",
      "Epoch [48/500], Loss: 1.0534\n",
      "Epoch [49/500], Loss: 1.0589\n",
      "Epoch [50/500], Loss: 1.0518\n",
      "Epoch [51/500], Loss: 1.0695\n",
      "Epoch [52/500], Loss: 1.0594\n",
      "Epoch [53/500], Loss: 1.0552\n",
      "Epoch [54/500], Loss: 1.0569\n",
      "Epoch [55/500], Loss: 1.0382\n",
      "Epoch [56/500], Loss: 1.0455\n",
      "Epoch [57/500], Loss: 1.0438\n",
      "Epoch [58/500], Loss: 1.0497\n",
      "Epoch [59/500], Loss: 1.0441\n",
      "Epoch [60/500], Loss: 1.0425\n",
      "Epoch [61/500], Loss: 1.0506\n",
      "Epoch [62/500], Loss: 1.0516\n",
      "Epoch [63/500], Loss: 1.0493\n",
      "Epoch [64/500], Loss: 1.0411\n",
      "Epoch [65/500], Loss: 1.0482\n",
      "Epoch [66/500], Loss: 1.0387\n",
      "Epoch [67/500], Loss: 1.0362\n",
      "Epoch [68/500], Loss: 1.0361\n",
      "Epoch [69/500], Loss: 1.0339\n",
      "Epoch [70/500], Loss: 1.0564\n",
      "Epoch [71/500], Loss: 1.0367\n",
      "Epoch [72/500], Loss: 1.0378\n",
      "Epoch [73/500], Loss: 1.0453\n",
      "Epoch [74/500], Loss: 1.0321\n",
      "Epoch [75/500], Loss: 1.0385\n",
      "Epoch [76/500], Loss: 1.0365\n",
      "Epoch [77/500], Loss: 1.0332\n",
      "Epoch [78/500], Loss: 1.0374\n",
      "Epoch [79/500], Loss: 1.0439\n",
      "Epoch [80/500], Loss: 1.0333\n",
      "Epoch [81/500], Loss: 1.0380\n",
      "Epoch [82/500], Loss: 1.0360\n",
      "Epoch [83/500], Loss: 1.0277\n",
      "Epoch [84/500], Loss: 1.0337\n",
      "Epoch [85/500], Loss: 1.0439\n",
      "Epoch [86/500], Loss: 1.0322\n",
      "Epoch [87/500], Loss: 1.0425\n",
      "Epoch [88/500], Loss: 1.0244\n",
      "Epoch [89/500], Loss: 1.0343\n",
      "Epoch [90/500], Loss: 1.0223\n",
      "Epoch [91/500], Loss: 1.0363\n",
      "Epoch [92/500], Loss: 1.0256\n",
      "Epoch [93/500], Loss: 1.0230\n",
      "Epoch [94/500], Loss: 1.0292\n",
      "Epoch [95/500], Loss: 1.0322\n",
      "Epoch [96/500], Loss: 1.0341\n",
      "Epoch [97/500], Loss: 1.0098\n",
      "Epoch [98/500], Loss: 1.0325\n",
      "Epoch [99/500], Loss: 1.0227\n",
      "Epoch [100/500], Loss: 1.0247\n",
      "Epoch [101/500], Loss: 1.0162\n",
      "Epoch [102/500], Loss: 1.0316\n",
      "Epoch [103/500], Loss: 1.0177\n",
      "Epoch [104/500], Loss: 1.0137\n",
      "Epoch [105/500], Loss: 1.0215\n",
      "Epoch [106/500], Loss: 1.0016\n",
      "Epoch [107/500], Loss: 1.0127\n",
      "Epoch [108/500], Loss: 1.0213\n",
      "Epoch [109/500], Loss: 1.0188\n",
      "Epoch [110/500], Loss: 1.0106\n",
      "Epoch [111/500], Loss: 1.0129\n",
      "Epoch [112/500], Loss: 1.0117\n",
      "Epoch [113/500], Loss: 1.0122\n",
      "Epoch [114/500], Loss: 1.0201\n",
      "Epoch [115/500], Loss: 1.0090\n",
      "Epoch [116/500], Loss: 1.0102\n",
      "Epoch [117/500], Loss: 1.0263\n",
      "Epoch [118/500], Loss: 1.0119\n",
      "Epoch [119/500], Loss: 1.0137\n",
      "Epoch [120/500], Loss: 1.0226\n",
      "Epoch [121/500], Loss: 1.0103\n",
      "Epoch [122/500], Loss: 1.0052\n",
      "Epoch [123/500], Loss: 1.0054\n",
      "Epoch [124/500], Loss: 1.0105\n",
      "Epoch [125/500], Loss: 0.9986\n",
      "Epoch [126/500], Loss: 0.9917\n",
      "Epoch [127/500], Loss: 0.9965\n",
      "Epoch [128/500], Loss: 0.9979\n",
      "Epoch [129/500], Loss: 1.0148\n",
      "Epoch [130/500], Loss: 1.0032\n",
      "Epoch [131/500], Loss: 1.0130\n",
      "Epoch [132/500], Loss: 1.0043\n",
      "Epoch [133/500], Loss: 1.0006\n",
      "Epoch [134/500], Loss: 1.0195\n",
      "Epoch [135/500], Loss: 0.9883\n",
      "Epoch [136/500], Loss: 1.0007\n",
      "Epoch [137/500], Loss: 0.9993\n",
      "Epoch [138/500], Loss: 1.0025\n",
      "Epoch [139/500], Loss: 1.0029\n",
      "Epoch [140/500], Loss: 0.9924\n",
      "Epoch [141/500], Loss: 1.0116\n",
      "Epoch [142/500], Loss: 0.9984\n",
      "Epoch [143/500], Loss: 0.9924\n",
      "Epoch [144/500], Loss: 0.9827\n",
      "Epoch [145/500], Loss: 0.9906\n",
      "Epoch [146/500], Loss: 0.9889\n",
      "Epoch [147/500], Loss: 0.9908\n",
      "Epoch [148/500], Loss: 0.9955\n",
      "Epoch [149/500], Loss: 0.9951\n",
      "Epoch [150/500], Loss: 0.9941\n",
      "Epoch [151/500], Loss: 1.0003\n",
      "Epoch [152/500], Loss: 0.9823\n",
      "Epoch [153/500], Loss: 0.9830\n",
      "Epoch [154/500], Loss: 0.9906\n",
      "Epoch [155/500], Loss: 0.9829\n",
      "Epoch [156/500], Loss: 0.9758\n",
      "Epoch [157/500], Loss: 0.9988\n",
      "Epoch [158/500], Loss: 0.9918\n",
      "Epoch [159/500], Loss: 0.9939\n",
      "Epoch [160/500], Loss: 0.9891\n",
      "Epoch [161/500], Loss: 0.9852\n",
      "Epoch [162/500], Loss: 1.0064\n",
      "Epoch [163/500], Loss: 0.9884\n",
      "Epoch [164/500], Loss: 0.9813\n",
      "Epoch [165/500], Loss: 0.9889\n",
      "Epoch [166/500], Loss: 0.9772\n",
      "Epoch [167/500], Loss: 0.9747\n",
      "Epoch [168/500], Loss: 0.9846\n",
      "Epoch [169/500], Loss: 0.9994\n",
      "Epoch [170/500], Loss: 0.9774\n",
      "Epoch [171/500], Loss: 0.9772\n",
      "Epoch [172/500], Loss: 0.9768\n",
      "Epoch [173/500], Loss: 0.9787\n",
      "Epoch [174/500], Loss: 0.9656\n",
      "Epoch [175/500], Loss: 0.9616\n",
      "Epoch [176/500], Loss: 0.9759\n",
      "Epoch [177/500], Loss: 0.9732\n",
      "Epoch [178/500], Loss: 0.9810\n",
      "Epoch [179/500], Loss: 0.9647\n",
      "Epoch [180/500], Loss: 0.9611\n",
      "Epoch [181/500], Loss: 0.9705\n",
      "Epoch [182/500], Loss: 0.9651\n",
      "Epoch [183/500], Loss: 0.9863\n",
      "Epoch [184/500], Loss: 0.9667\n",
      "Epoch [185/500], Loss: 0.9862\n",
      "Epoch [186/500], Loss: 0.9712\n",
      "Epoch [187/500], Loss: 0.9745\n",
      "Epoch [188/500], Loss: 0.9551\n",
      "Epoch [189/500], Loss: 0.9871\n",
      "Epoch [190/500], Loss: 0.9692\n",
      "Epoch [191/500], Loss: 0.9806\n",
      "Epoch [192/500], Loss: 0.9558\n",
      "Epoch [193/500], Loss: 0.9638\n",
      "Epoch [194/500], Loss: 0.9644\n",
      "Epoch [195/500], Loss: 0.9613\n",
      "Epoch [196/500], Loss: 0.9486\n",
      "Epoch [197/500], Loss: 0.9689\n",
      "Epoch [198/500], Loss: 0.9759\n",
      "Epoch [199/500], Loss: 0.9551\n",
      "Epoch [200/500], Loss: 0.9673\n",
      "Epoch [201/500], Loss: 0.9507\n",
      "Epoch [202/500], Loss: 0.9537\n",
      "Epoch [203/500], Loss: 0.9656\n",
      "Epoch [204/500], Loss: 0.9548\n",
      "Epoch [205/500], Loss: 0.9569\n",
      "Epoch [206/500], Loss: 0.9574\n",
      "Epoch [207/500], Loss: 0.9457\n",
      "Epoch [208/500], Loss: 0.9734\n",
      "Epoch [209/500], Loss: 0.9814\n",
      "Epoch [210/500], Loss: 0.9806\n",
      "Epoch [211/500], Loss: 0.9432\n",
      "Epoch [212/500], Loss: 0.9535\n",
      "Epoch [213/500], Loss: 0.9429\n",
      "Epoch [214/500], Loss: 0.9508\n",
      "Epoch [215/500], Loss: 0.9634\n",
      "Epoch [216/500], Loss: 0.9462\n",
      "Epoch [217/500], Loss: 0.9442\n",
      "Epoch [218/500], Loss: 0.9504\n",
      "Epoch [219/500], Loss: 0.9439\n",
      "Epoch [220/500], Loss: 0.9573\n",
      "Epoch [221/500], Loss: 0.9565\n",
      "Epoch [222/500], Loss: 0.9399\n",
      "Epoch [223/500], Loss: 0.9559\n",
      "Epoch [224/500], Loss: 0.9492\n",
      "Epoch [225/500], Loss: 0.9424\n",
      "Epoch [226/500], Loss: 0.9400\n",
      "Epoch [227/500], Loss: 0.9505\n",
      "Epoch [228/500], Loss: 0.9434\n",
      "Epoch [229/500], Loss: 0.9470\n",
      "Epoch [230/500], Loss: 0.9652\n",
      "Epoch [231/500], Loss: 0.9437\n",
      "Epoch [232/500], Loss: 0.9604\n",
      "Epoch [233/500], Loss: 0.9444\n",
      "Epoch [234/500], Loss: 0.9326\n",
      "Epoch [235/500], Loss: 0.9347\n",
      "Epoch [236/500], Loss: 0.9248\n",
      "Epoch [237/500], Loss: 0.9360\n",
      "Epoch [238/500], Loss: 0.9311\n",
      "Epoch [239/500], Loss: 0.9192\n",
      "Epoch [240/500], Loss: 0.9304\n",
      "Epoch [241/500], Loss: 0.9214\n",
      "Epoch [242/500], Loss: 0.9141\n",
      "Epoch [243/500], Loss: 0.9343\n",
      "Epoch [244/500], Loss: 0.9444\n",
      "Epoch [245/500], Loss: 0.9551\n",
      "Epoch [246/500], Loss: 0.9133\n",
      "Epoch [247/500], Loss: 0.9380\n",
      "Epoch [248/500], Loss: 0.9440\n",
      "Epoch [249/500], Loss: 0.9405\n",
      "Epoch [250/500], Loss: 0.9378\n",
      "Epoch [251/500], Loss: 0.9345\n",
      "Epoch [252/500], Loss: 0.9218\n",
      "Epoch [253/500], Loss: 0.9373\n",
      "Epoch [254/500], Loss: 0.9265\n",
      "Epoch [255/500], Loss: 0.9391\n",
      "Epoch [256/500], Loss: 0.9200\n",
      "Epoch [257/500], Loss: 0.9161\n",
      "Epoch [258/500], Loss: 0.9148\n",
      "Epoch [259/500], Loss: 0.9210\n",
      "Epoch [260/500], Loss: 0.9278\n",
      "Epoch [261/500], Loss: 0.9349\n",
      "Epoch [262/500], Loss: 0.9275\n",
      "Epoch [263/500], Loss: 0.9169\n",
      "Epoch [264/500], Loss: 0.9251\n",
      "Epoch [265/500], Loss: 0.9249\n",
      "Epoch [266/500], Loss: 0.9370\n",
      "Epoch [267/500], Loss: 0.9259\n",
      "Epoch [268/500], Loss: 0.9379\n",
      "Epoch [269/500], Loss: 0.9156\n",
      "Epoch [270/500], Loss: 0.9165\n",
      "Epoch [271/500], Loss: 0.9235\n",
      "Epoch [272/500], Loss: 0.9256\n",
      "Epoch [273/500], Loss: 0.9263\n",
      "Epoch [274/500], Loss: 0.9162\n",
      "Epoch [275/500], Loss: 0.9186\n",
      "Epoch [276/500], Loss: 0.9288\n",
      "Epoch [277/500], Loss: 0.9186\n",
      "Epoch [278/500], Loss: 0.9004\n",
      "Epoch [279/500], Loss: 0.9088\n",
      "Epoch [280/500], Loss: 0.9152\n",
      "Epoch [281/500], Loss: 0.9273\n",
      "Epoch [282/500], Loss: 0.8921\n",
      "Epoch [283/500], Loss: 0.9284\n",
      "Epoch [284/500], Loss: 0.9046\n",
      "Epoch [285/500], Loss: 0.9256\n",
      "Epoch [286/500], Loss: 0.9270\n",
      "Epoch [287/500], Loss: 0.8981\n",
      "Epoch [288/500], Loss: 0.9086\n",
      "Epoch [289/500], Loss: 0.9101\n",
      "Epoch [290/500], Loss: 0.9193\n",
      "Epoch [291/500], Loss: 0.8997\n",
      "Epoch [292/500], Loss: 0.9142\n",
      "Epoch [293/500], Loss: 0.9344\n",
      "Epoch [294/500], Loss: 0.8921\n",
      "Epoch [295/500], Loss: 0.9330\n",
      "Epoch [296/500], Loss: 0.9113\n",
      "Epoch [297/500], Loss: 0.9191\n",
      "Epoch [298/500], Loss: 0.9086\n",
      "Epoch [299/500], Loss: 0.9019\n",
      "Epoch [300/500], Loss: 0.9309\n",
      "Epoch [301/500], Loss: 0.9058\n",
      "Epoch [302/500], Loss: 0.9115\n",
      "Epoch [303/500], Loss: 0.9025\n",
      "Epoch [304/500], Loss: 0.8994\n",
      "Epoch [305/500], Loss: 0.8977\n",
      "Epoch [306/500], Loss: 0.9196\n",
      "Epoch [307/500], Loss: 0.8950\n",
      "Epoch [308/500], Loss: 0.9300\n",
      "Epoch [309/500], Loss: 0.9121\n",
      "Epoch [310/500], Loss: 0.9204\n",
      "Epoch [311/500], Loss: 0.9057\n",
      "Epoch [312/500], Loss: 0.9264\n",
      "Epoch [313/500], Loss: 0.8898\n",
      "Epoch [314/500], Loss: 0.8998\n",
      "Epoch [315/500], Loss: 0.8942\n",
      "Epoch [316/500], Loss: 0.8915\n",
      "Epoch [317/500], Loss: 0.9030\n",
      "Epoch [318/500], Loss: 0.8928\n",
      "Epoch [319/500], Loss: 0.8825\n",
      "Epoch [320/500], Loss: 0.9058\n",
      "Epoch [321/500], Loss: 0.9033\n",
      "Epoch [322/500], Loss: 0.8851\n",
      "Epoch [323/500], Loss: 0.9321\n",
      "Epoch [324/500], Loss: 0.8989\n",
      "Epoch [325/500], Loss: 0.8986\n",
      "Epoch [326/500], Loss: 0.8954\n",
      "Epoch [327/500], Loss: 0.8837\n",
      "Epoch [328/500], Loss: 0.8718\n",
      "Epoch [329/500], Loss: 0.9268\n",
      "Epoch [330/500], Loss: 0.8868\n",
      "Epoch [331/500], Loss: 0.8772\n",
      "Epoch [332/500], Loss: 0.8782\n",
      "Epoch [333/500], Loss: 0.9317\n",
      "Epoch [334/500], Loss: 0.8838\n",
      "Epoch [335/500], Loss: 0.8952\n",
      "Epoch [336/500], Loss: 0.9043\n",
      "Epoch [337/500], Loss: 0.8798\n",
      "Epoch [338/500], Loss: 0.9026\n",
      "Epoch [339/500], Loss: 0.9140\n",
      "Epoch [340/500], Loss: 0.8853\n",
      "Epoch [341/500], Loss: 0.8927\n",
      "Epoch [342/500], Loss: 0.8848\n",
      "Epoch [343/500], Loss: 0.8872\n",
      "Epoch [344/500], Loss: 0.9071\n",
      "Epoch [345/500], Loss: 0.8799\n",
      "Epoch [346/500], Loss: 0.8861\n",
      "Epoch [347/500], Loss: 0.8776\n",
      "Epoch [348/500], Loss: 0.9125\n",
      "Epoch [349/500], Loss: 0.8852\n",
      "Epoch [350/500], Loss: 0.8568\n",
      "Epoch [351/500], Loss: 0.8947\n",
      "Epoch [352/500], Loss: 0.8984\n",
      "Epoch [353/500], Loss: 0.8943\n",
      "Epoch [354/500], Loss: 0.8763\n",
      "Epoch [355/500], Loss: 0.8927\n",
      "Epoch [356/500], Loss: 0.8826\n",
      "Epoch [357/500], Loss: 0.8943\n",
      "Epoch [358/500], Loss: 0.8939\n",
      "Epoch [359/500], Loss: 0.8805\n",
      "Epoch [360/500], Loss: 0.8908\n",
      "Epoch [361/500], Loss: 0.8679\n",
      "Epoch [362/500], Loss: 0.8667\n",
      "Epoch [363/500], Loss: 0.8894\n",
      "Epoch [364/500], Loss: 0.8931\n",
      "Epoch [365/500], Loss: 0.8775\n",
      "Epoch [366/500], Loss: 0.8728\n",
      "Epoch [367/500], Loss: 0.8953\n",
      "Epoch [368/500], Loss: 0.8865\n",
      "Epoch [369/500], Loss: 0.8855\n",
      "Epoch [370/500], Loss: 0.8859\n",
      "Epoch [371/500], Loss: 0.8778\n",
      "Epoch [372/500], Loss: 0.8662\n",
      "Epoch [373/500], Loss: 0.8660\n",
      "Epoch [374/500], Loss: 0.8678\n",
      "Epoch [375/500], Loss: 0.8669\n",
      "Epoch [376/500], Loss: 0.8936\n",
      "Epoch [377/500], Loss: 0.8924\n",
      "Epoch [378/500], Loss: 0.8851\n",
      "Epoch [379/500], Loss: 0.8757\n",
      "Epoch [380/500], Loss: 0.8641\n",
      "Epoch [381/500], Loss: 0.8856\n",
      "Epoch [382/500], Loss: 0.8708\n",
      "Epoch [383/500], Loss: 0.8920\n",
      "Epoch [384/500], Loss: 0.8653\n",
      "Epoch [385/500], Loss: 0.8744\n",
      "Epoch [386/500], Loss: 0.8830\n",
      "Epoch [387/500], Loss: 0.8745\n",
      "Epoch [388/500], Loss: 0.8647\n",
      "Epoch [389/500], Loss: 0.8681\n",
      "Epoch [390/500], Loss: 0.8656\n",
      "Epoch [391/500], Loss: 0.8571\n",
      "Epoch [392/500], Loss: 0.8835\n",
      "Epoch [393/500], Loss: 0.8867\n",
      "Epoch [394/500], Loss: 0.8920\n",
      "Epoch [395/500], Loss: 0.8683\n",
      "Epoch [396/500], Loss: 0.8522\n",
      "Epoch [397/500], Loss: 0.8468\n",
      "Epoch [398/500], Loss: 0.8663\n",
      "Epoch [399/500], Loss: 0.8737\n",
      "Epoch [400/500], Loss: 0.8308\n",
      "Epoch [401/500], Loss: 0.8565\n",
      "Epoch [402/500], Loss: 0.8487\n",
      "Epoch [403/500], Loss: 0.8673\n",
      "Epoch [404/500], Loss: 0.8691\n",
      "Epoch [405/500], Loss: 0.8737\n",
      "Epoch [406/500], Loss: 0.8595\n",
      "Epoch [407/500], Loss: 0.8517\n",
      "Epoch [408/500], Loss: 0.8517\n",
      "Epoch [409/500], Loss: 0.8538\n",
      "Epoch [410/500], Loss: 0.8769\n",
      "Epoch [411/500], Loss: 0.8636\n",
      "Epoch [412/500], Loss: 0.8685\n",
      "Epoch [413/500], Loss: 0.8593\n",
      "Epoch [414/500], Loss: 0.8596\n",
      "Epoch [415/500], Loss: 0.8566\n",
      "Epoch [416/500], Loss: 0.8547\n",
      "Epoch [417/500], Loss: 0.8549\n",
      "Epoch [418/500], Loss: 0.8478\n",
      "Epoch [419/500], Loss: 0.8409\n",
      "Epoch [420/500], Loss: 0.8609\n",
      "Epoch [421/500], Loss: 0.8427\n",
      "Epoch [422/500], Loss: 0.8507\n",
      "Epoch [423/500], Loss: 0.8490\n",
      "Epoch [424/500], Loss: 0.8655\n",
      "Epoch [425/500], Loss: 0.8551\n",
      "Epoch [426/500], Loss: 0.8660\n",
      "Epoch [427/500], Loss: 0.8487\n",
      "Epoch [428/500], Loss: 0.8271\n",
      "Epoch [429/500], Loss: 0.8517\n",
      "Epoch [430/500], Loss: 0.8427\n",
      "Epoch [431/500], Loss: 0.8596\n",
      "Epoch [432/500], Loss: 0.8498\n",
      "Epoch [433/500], Loss: 0.8595\n",
      "Epoch [434/500], Loss: 0.8654\n",
      "Epoch [435/500], Loss: 0.8845\n",
      "Epoch [436/500], Loss: 0.8368\n",
      "Epoch [437/500], Loss: 0.8487\n",
      "Epoch [438/500], Loss: 0.8321\n",
      "Epoch [439/500], Loss: 0.8289\n",
      "Epoch [440/500], Loss: 0.8342\n",
      "Epoch [441/500], Loss: 0.8579\n",
      "Epoch [442/500], Loss: 0.8307\n",
      "Epoch [443/500], Loss: 0.8401\n",
      "Epoch [444/500], Loss: 0.8459\n",
      "Epoch [445/500], Loss: 0.8540\n",
      "Epoch [446/500], Loss: 0.8462\n",
      "Epoch [447/500], Loss: 0.8487\n",
      "Epoch [448/500], Loss: 0.8352\n",
      "Epoch [449/500], Loss: 0.8416\n",
      "Epoch [450/500], Loss: 0.8465\n",
      "Epoch [451/500], Loss: 0.8493\n",
      "Epoch [452/500], Loss: 0.8282\n",
      "Epoch [453/500], Loss: 0.8216\n",
      "Epoch [454/500], Loss: 0.8435\n",
      "Epoch [455/500], Loss: 0.8577\n",
      "Epoch [456/500], Loss: 0.8306\n",
      "Epoch [457/500], Loss: 0.8414\n",
      "Epoch [458/500], Loss: 0.8087\n",
      "Epoch [459/500], Loss: 0.8478\n",
      "Epoch [460/500], Loss: 0.8426\n",
      "Epoch [461/500], Loss: 0.8156\n",
      "Epoch [462/500], Loss: 0.8262\n",
      "Epoch [463/500], Loss: 0.8306\n",
      "Epoch [464/500], Loss: 0.8446\n",
      "Epoch [465/500], Loss: 0.8505\n",
      "Epoch [466/500], Loss: 0.8265\n",
      "Epoch [467/500], Loss: 0.8704\n",
      "Epoch [468/500], Loss: 0.8400\n",
      "Epoch [469/500], Loss: 0.8277\n",
      "Epoch [470/500], Loss: 0.8376\n",
      "Epoch [471/500], Loss: 0.8321\n",
      "Epoch [472/500], Loss: 0.8412\n",
      "Epoch [473/500], Loss: 0.8416\n",
      "Epoch [474/500], Loss: 0.8179\n",
      "Epoch [475/500], Loss: 0.8370\n",
      "Epoch [476/500], Loss: 0.8494\n",
      "Epoch [477/500], Loss: 0.8437\n",
      "Epoch [478/500], Loss: 0.8239\n",
      "Epoch [479/500], Loss: 0.8321\n",
      "Epoch [480/500], Loss: 0.8385\n",
      "Epoch [481/500], Loss: 0.8298\n",
      "Epoch [482/500], Loss: 0.8237\n",
      "Epoch [483/500], Loss: 0.8445\n",
      "Epoch [484/500], Loss: 0.8380\n",
      "Epoch [485/500], Loss: 0.8386\n",
      "Epoch [486/500], Loss: 0.8449\n",
      "Epoch [487/500], Loss: 0.8494\n",
      "Epoch [488/500], Loss: 0.8466\n",
      "Epoch [489/500], Loss: 0.8205\n",
      "Epoch [490/500], Loss: 0.8365\n",
      "Epoch [491/500], Loss: 0.8271\n",
      "Epoch [492/500], Loss: 0.8291\n",
      "Epoch [493/500], Loss: 0.8271\n",
      "Epoch [494/500], Loss: 0.8206\n",
      "Epoch [495/500], Loss: 0.8230\n",
      "Epoch [496/500], Loss: 0.8227\n",
      "Epoch [497/500], Loss: 0.8183\n",
      "Epoch [498/500], Loss: 0.8283\n",
      "Epoch [499/500], Loss: 0.8318\n",
      "Epoch [500/500], Loss: 0.8508\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "mlp = MLP(768,256,3)\n",
    "mlp.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(mlp.parameters(),lr=learning_rate)\n",
    "n_epochs = 500\n",
    "train_loader = DataLoader(se_train,batch_size=64,shuffle=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    mlp.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(batch[\"embedding\"].to(device))\n",
    "        loss = criterion(outputs,torch.tensor(batch[\"label\"]).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 51.30%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "mlp.eval()\n",
    "correct, total = 0, 0\n",
    "se_test = snliEmbeddings(split=\"test\")\n",
    "test_loader = DataLoader(se_test,batch_size=32)\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = mlp(batch[\"embedding\"].to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # print(predicted, batch[\"label\"])\n",
    "        total += batch[\"label\"].size(0)\n",
    "        correct += (predicted == batch[\"label\"].to(device)).sum().item()\n",
    "    \n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  2048,  2308,  2024, 23581,  2096,  3173,  2000,  2175, 14555,\n",
      "         1012,   102,  1996,  5208,  2024, 17662,  9119,  2096,  3173,  2000,\n",
      "         2175, 14555,  2044,  2074,  5983,  6265,  1012,   102]), 'embedding': tensor([-4.9645e-01, -1.7457e-01, -8.3766e-01,  7.9489e-01,  7.5655e-01,\n",
      "        -2.4049e-01,  5.3153e-01,  1.7647e-01, -1.8890e-01, -1.1814e+00,\n",
      "        -3.0919e-01,  1.0288e+00,  1.0833e+00,  7.8304e-02,  1.0192e+00,\n",
      "        -4.1620e-01, -4.0028e-01, -1.1613e-01,  1.2654e-01, -6.3370e-02,\n",
      "         9.0735e-01,  3.1044e+00, -1.5455e-01,  3.5114e-02,  9.9734e-02,\n",
      "         1.2293e+00, -5.2205e-01,  1.0538e+00,  8.8199e-01,  5.2332e-01,\n",
      "        -4.4482e-01, -1.2841e-02, -1.4690e+00, -1.2647e-01, -1.2444e+00,\n",
      "        -1.0411e+00,  1.4478e-01, -2.5737e-01, -1.3852e-01, -4.3224e-02,\n",
      "        -1.0746e+00,  8.0413e-02,  1.5950e+00,  1.3573e-01,  5.4307e-01,\n",
      "        -7.0265e-03, -3.0560e+00,  2.2700e-01, -6.7728e-01,  2.1787e-01,\n",
      "         5.6539e-01,  6.5040e-01, -2.0095e-02,  2.3859e-01,  1.7955e-01,\n",
      "        -4.9576e-01,  1.0532e-01,  2.0079e-01, -5.3467e-03, -2.1579e-01,\n",
      "        -1.7516e-01,  7.0900e-02, -1.0172e+00, -4.3551e-01,  7.0810e-01,\n",
      "         3.4080e-01, -2.9435e-01, -1.8807e-01, -1.3331e-01, -1.2767e-01,\n",
      "         6.2191e-01,  2.3478e-01,  2.5318e-01, -9.1784e-01,  6.7780e-02,\n",
      "         2.3027e-01,  4.1177e-02,  5.1141e+00, -2.6229e-01, -1.1965e+00,\n",
      "         1.2929e+00,  1.4343e-01, -9.6857e-02,  9.1758e-02, -1.0442e-01,\n",
      "        -4.1717e+00,  2.3565e-01, -1.5853e-01, -1.2678e+00,  8.5713e-02,\n",
      "         6.4277e-01, -3.5107e-02,  7.0659e-01, -1.2366e-01, -3.9707e-02,\n",
      "        -5.5753e-01, -2.2448e-01, -7.9111e-01, -1.5352e-01, -3.2007e-01,\n",
      "         2.1753e-01, -1.5411e-01, -2.1832e-01, -1.4302e-01,  1.6757e-01,\n",
      "        -1.9870e-01, -3.2977e-01,  4.2092e-02,  2.2125e-01,  2.7632e-01,\n",
      "         1.7873e-02, -4.3396e-02,  1.9908e-01, -7.3545e-01,  9.0596e-02,\n",
      "        -6.6643e-02, -1.1186e+00,  2.0614e-02, -1.2588e+00,  6.1681e-01,\n",
      "        -5.3803e-01, -3.3256e-01,  8.9326e-01, -4.2870e-01,  2.1176e-01,\n",
      "        -1.1004e-01, -4.5750e-01, -5.6526e+00, -2.7082e-01, -6.4920e-01,\n",
      "        -1.9099e-01, -1.4802e-01, -1.0417e+00, -9.8649e-01,  2.5556e-01,\n",
      "         6.4040e-01,  1.5881e-01,  1.3443e+00, -2.9971e-01,  9.9309e-01,\n",
      "        -8.1536e-02, -6.6893e-01,  5.6035e-01, -2.1010e-01,  7.7152e-01,\n",
      "         6.3730e-02, -3.8146e-01,  5.9898e-02, -4.2029e-01,  2.6074e-01,\n",
      "        -8.6079e-01, -1.9943e-01, -2.1186e-01, -7.8614e-01,  1.3383e-04,\n",
      "         1.0487e+00, -8.3020e-01, -4.1477e-01,  6.3617e-02, -1.1448e-01,\n",
      "        -4.6063e-01,  5.0481e-01,  8.3147e-01,  2.4186e-01, -2.1729e-01,\n",
      "         5.0748e-02, -2.5916e-01,  2.7904e-01, -3.4352e-01, -3.0559e-01,\n",
      "         1.6147e-01, -1.5069e-01, -7.9137e-01, -1.2266e+00, -2.0730e-01,\n",
      "         1.0635e-01,  1.2639e+00,  4.4771e-01,  1.7167e-01,  2.0132e-01,\n",
      "        -9.5119e-03,  2.6026e-01, -1.0764e+00,  1.1381e+00, -1.9735e-01,\n",
      "         8.4823e-02, -8.6184e-01,  2.0977e-01, -5.1068e-01,  1.4960e-01,\n",
      "         5.4025e-01, -5.2985e-01, -2.4792e-01, -1.9648e-01, -7.2595e-02,\n",
      "        -9.5419e-02, -1.3229e+00,  2.8031e-01, -1.9259e-01, -1.7972e-01,\n",
      "        -1.4333e-03,  8.6333e-01,  7.5552e-01,  4.5510e-01,  4.2451e-01,\n",
      "         3.3676e-01, -6.6377e-01, -4.1439e-01,  4.6152e-02,  7.0501e-02,\n",
      "         2.0960e-01,  1.2158e+00, -1.0642e+00, -1.2179e-01, -5.2787e-01,\n",
      "        -1.1978e+00,  9.8216e-03, -6.2124e-01, -1.1756e-01, -3.3879e-01,\n",
      "         4.1214e-01, -1.0380e+00, -7.4137e-02,  1.6188e-01, -1.0209e+00,\n",
      "        -7.0209e-01,  1.6332e-01, -7.5897e-02,  2.1145e-01, -4.5924e-02,\n",
      "         8.7820e-01,  8.8039e-01, -2.4064e-01,  2.4675e-01,  1.0779e+00,\n",
      "        -1.3773e+00, -6.9741e-01,  6.2854e-01, -1.2438e-01,  3.5840e-01,\n",
      "        -2.7275e-01,  6.0400e-01,  1.0195e+00,  5.6185e-01, -9.9825e-01,\n",
      "        -5.9528e-01, -1.3473e-01, -1.8419e-01, -2.0638e-01, -3.3291e-01,\n",
      "         1.0682e-01, -1.2586e-01,  2.1981e-01,  2.6595e-01, -4.4656e-01,\n",
      "         1.0466e+00, -9.1100e-01, -1.0099e+00, -1.1046e+00,  5.9343e-02,\n",
      "        -1.2713e+00,  9.1860e-01,  1.1772e-01,  1.4167e+00, -3.0820e-01,\n",
      "        -3.8858e-01, -1.2082e+00,  5.9681e-01,  1.5329e-01,  8.3574e-01,\n",
      "        -7.6357e-01, -7.2579e-01, -2.5681e-01, -1.1836e+00,  2.1704e-02,\n",
      "        -1.5784e-01, -1.3215e-01,  1.0896e-01, -7.5596e-01,  1.4213e-01,\n",
      "         4.1099e-01,  2.2584e-01, -6.8373e-01,  1.2605e+00,  4.3385e+00,\n",
      "         9.3564e-01,  7.0147e-01,  2.2593e-01, -2.6296e+00, -7.2787e-01,\n",
      "         1.1334e+00, -1.4458e+00, -4.4725e+00, -7.4245e-01, -5.9255e-01,\n",
      "         2.2220e-01, -5.5114e+00, -3.5283e-02, -1.3647e-01, -7.1127e-01,\n",
      "         2.8928e-01,  9.5026e-01,  9.4367e-01, -5.2830e+00,  1.3973e-01,\n",
      "         6.5297e-01,  9.6890e-02,  1.1784e+00, -3.7441e-01,  1.0350e+00,\n",
      "         3.6723e-01,  3.6577e-01, -2.3005e-01,  2.5165e-01, -1.0026e+00,\n",
      "        -6.3579e-01, -4.8472e-01, -8.3870e-01,  2.3110e+00,  3.8662e-02,\n",
      "        -2.5774e-01, -8.0053e-01,  4.4157e-01, -1.2392e-01,  3.7749e-01,\n",
      "        -1.0662e+00, -2.0726e-01,  6.5376e-01,  6.5684e-01,  1.2073e-01,\n",
      "         1.0789e-01, -1.7629e-01,  2.2560e-01,  1.4391e-01, -7.0333e-03,\n",
      "        -8.3581e-02, -3.2367e-01, -9.5562e-02,  7.7128e-02,  1.5578e-01,\n",
      "        -3.3115e-01, -1.0359e+00,  9.6355e-01, -1.8930e-01,  7.4397e-01,\n",
      "         5.4966e+00,  8.7215e-01, -7.2993e-01,  7.9107e-01,  1.4968e-01,\n",
      "        -8.2173e-01,  4.6275e+00,  4.1266e-01, -1.2510e+00,  6.2802e-03,\n",
      "         4.8472e-01, -3.0392e-01, -3.1025e-01,  9.5792e-01, -1.2601e-01,\n",
      "        -3.2059e-01, -7.7359e-01,  1.3555e+00, -1.2665e+00,  1.6780e+00,\n",
      "        -1.2288e-01, -1.0794e+00,  9.2451e-01,  9.8915e-01, -5.3787e-01,\n",
      "        -1.5203e-01,  2.3873e-01, -3.4607e-02,  2.1619e-01, -4.6905e-01,\n",
      "         4.6356e-01,  4.0870e-01, -7.0301e-02,  8.0767e-01,  1.2946e-03,\n",
      "         1.6502e-01,  3.7690e-02, -5.1794e-01, -1.2627e-01,  1.1616e+00,\n",
      "         2.4702e-01, -1.8451e-01,  1.5811e-02, -2.3894e-01, -5.4085e-01,\n",
      "        -7.3733e-01,  8.1391e-01,  4.9444e+00, -2.4335e-01,  1.3797e+00,\n",
      "         1.3468e-01, -5.7975e-02,  2.2278e-01,  8.4600e-02,  2.2939e-01,\n",
      "        -1.7362e-01, -3.0650e-01,  8.6056e-01, -9.5684e-01, -1.3760e+00,\n",
      "         5.7543e-01,  1.7583e-01, -4.6316e-02,  2.3583e+00,  3.9713e-01,\n",
      "         1.2835e-01,  2.7529e-01,  1.2200e+00,  2.0795e-01,  1.5238e-01,\n",
      "         5.7802e-01,  1.0496e+00, -1.0213e-01, -1.4254e-01,  4.1181e-01,\n",
      "        -2.6019e-01, -1.6401e-01, -1.9956e-01,  7.4715e-02, -9.4105e-01,\n",
      "         3.3660e-02, -9.6212e-01,  1.1209e+00,  8.1763e-01,  1.1415e-01,\n",
      "         2.1712e-01,  9.4175e-01,  5.2522e+00, -1.0014e+00,  4.3227e-01,\n",
      "        -4.8020e-02,  5.9406e-01, -2.4101e+00, -4.8224e-01, -2.2096e-01,\n",
      "        -1.0438e-01, -1.3921e-01, -3.9687e-02,  1.0258e-01, -7.9353e-01,\n",
      "         2.5823e-01,  8.5863e-01, -1.1551e+00, -1.2162e+00, -1.1913e-01,\n",
      "         6.6789e-01,  1.9997e-01, -1.9404e+00, -4.2864e-01, -1.1806e-01,\n",
      "         4.8667e-01, -1.6325e-01, -9.8339e-01, -1.9308e-02, -1.8012e-01,\n",
      "         1.3817e-01, -1.5382e-01, -7.5068e-02,  3.0209e-01,  5.2407e-01,\n",
      "        -1.1537e+00, -6.3387e-01, -1.7850e-01, -5.3861e-01,  3.6259e-01,\n",
      "        -1.7410e-01, -1.3665e+00,  1.3807e-02,  5.2380e+00, -3.6209e-01,\n",
      "         5.5717e-01,  5.1155e-01,  4.5069e-01, -1.2830e-01,  8.5621e-02,\n",
      "         1.2221e+00,  2.9936e-01,  3.6708e-02, -5.6448e-01, -2.1017e-01,\n",
      "        -2.9899e-01,  1.2188e-01,  1.1069e+00,  2.0881e-01,  7.4337e-01,\n",
      "         7.6721e-01,  1.5127e-01, -1.7735e-01,  1.0346e-01,  1.1925e+00,\n",
      "        -1.9601e-01, -1.0883e-01, -2.2063e-01, -1.8223e-01, -2.0039e-01,\n",
      "        -1.0632e-01,  4.9732e+00, -2.0267e-02,  9.1134e-01, -1.3641e+00,\n",
      "        -1.0076e+00, -4.2839e-01,  4.2699e+00,  9.4354e-01,  1.9674e-02,\n",
      "         3.4769e-01,  3.1041e-01, -1.8786e-02,  4.3406e-01, -7.2378e-02,\n",
      "        -9.3361e-02,  1.9314e-01,  1.4393e-01,  1.0404e+00, -4.4337e-01,\n",
      "        -1.2562e+00, -3.5869e-01,  1.4801e-01, -1.0035e+00,  3.1531e+00,\n",
      "        -1.5376e-01, -1.9485e-01, -1.4091e-01, -5.5086e-01,  2.3471e-01,\n",
      "         7.8171e-02, -1.0949e+00,  8.3101e-03,  2.6475e-01,  1.1654e+00,\n",
      "         8.3438e-02,  8.2161e-02, -8.5517e-01,  7.7288e-01,  6.5011e-01,\n",
      "        -7.8605e-01, -1.0514e+00,  1.3696e+00, -9.9779e-01,  1.8793e-01,\n",
      "         3.8943e+00,  9.2578e-02,  2.5554e-02,  1.9210e-01, -2.0667e-01,\n",
      "         1.3330e-01, -8.0349e-01,  3.6825e-01, -9.5498e-01, -1.9901e-01,\n",
      "        -1.1542e-01,  2.2479e-01, -4.8591e-02, -7.2700e-01,  5.7673e-01,\n",
      "         8.3076e-02,  1.4793e-01, -2.4995e-01, -1.6370e-01,  3.2205e-01,\n",
      "         5.2713e-01, -2.1289e-01, -6.6589e-02,  9.3009e-02, -6.9050e-02,\n",
      "        -1.1391e+00, -2.6675e-01, -2.4664e-01, -3.2579e+00,  3.6349e-01,\n",
      "        -5.2449e+00,  8.6390e-01, -1.9387e-01, -1.8010e-01,  8.8485e-01,\n",
      "         5.3851e-01,  6.6678e-01, -8.3292e-01, -3.1528e-01,  3.8887e-01,\n",
      "         6.2177e-01, -2.6032e-01, -1.9956e-01, -6.0572e-01,  2.6731e-01,\n",
      "        -1.4893e-01,  2.0566e-01, -4.5046e-01,  2.6822e-01, -2.0971e-01,\n",
      "         5.2169e+00,  9.8336e-02, -4.5078e-01, -9.0561e-01,  8.3809e-02,\n",
      "        -1.3772e-01,  4.4407e+00, -4.4129e-01, -1.1413e+00,  2.6358e-01,\n",
      "        -6.5916e-01, -6.6365e-01,  2.8244e-01,  1.3519e-01, -6.8345e-01,\n",
      "        -1.3508e+00,  9.0286e-01,  4.8385e-01, -3.1176e-02,  8.1489e-01,\n",
      "        -2.0115e-01, -3.0068e-01, -4.0496e-03,  6.9428e-01,  1.3005e+00,\n",
      "         8.8914e-01,  5.6400e-01, -9.0829e-01, -7.4005e-01,  1.0136e+00,\n",
      "         1.0364e-01,  2.7230e-01, -3.1003e-02,  4.5394e+00,  2.1354e-01,\n",
      "        -7.5570e-01, -5.4316e-01, -7.5405e-01, -2.0123e-01, -4.5727e-01,\n",
      "         9.7162e-02,  2.6631e-01,  9.2683e-01, -1.4722e-01,  1.1405e+00,\n",
      "        -7.2917e-01,  4.5605e-02, -5.8999e-01, -6.0255e-02,  2.7466e-01,\n",
      "        -1.1265e+00, -1.1510e+00, -1.3095e+00,  5.6096e-01, -5.8898e-02,\n",
      "        -1.3085e-01,  1.2383e-01,  1.3327e-02,  2.9050e-01,  2.4046e-01,\n",
      "        -4.8157e+00,  9.1966e-01,  1.0571e-01,  6.0091e-01,  1.1060e+00,\n",
      "         6.5898e-01,  5.2380e-01,  1.7580e-01, -1.1089e+00, -1.2423e+00,\n",
      "        -2.7343e-01, -1.1509e-03,  4.7367e-01,  3.5601e-01,  7.2790e-01,\n",
      "         1.6404e-01, -6.8011e-02, -5.9706e-01, -6.1307e-01, -8.3530e-01,\n",
      "        -1.2520e+00,  1.5303e-01,  1.7123e-01, -4.9594e-01,  1.0552e+00,\n",
      "        -1.1824e-01,  2.6316e-02,  3.2224e-02, -7.4457e-01,  4.1770e-01,\n",
      "         3.3353e-01,  2.4793e-01,  2.6749e-02,  4.7241e-01,  7.4458e-01,\n",
      "         3.9871e-01,  9.2413e-01, -5.7454e-01,  4.0607e-01, -5.7249e-01,\n",
      "         3.5465e-01,  6.7043e-01, -6.3948e-01,  2.2750e-01,  7.5131e-01,\n",
      "        -7.5295e-02,  1.3593e-01, -9.9647e-02, -7.2428e-01,  4.8731e-01,\n",
      "        -4.2276e-02,  7.2655e-01, -3.4952e-01, -1.1590e-01, -1.5707e-01,\n",
      "         5.1035e-02, -7.2290e-02, -6.2654e-01, -3.8953e-02,  2.9800e-01,\n",
      "         8.7302e-01,  1.1193e+00, -7.1820e-02, -2.5741e-01, -2.2316e-01,\n",
      "         4.6104e-03, -9.1505e-01,  3.7806e-01, -1.0429e-01,  3.8511e-01,\n",
      "         1.0316e+00,  9.7685e-02,  8.8796e-01,  2.4181e-01, -1.3879e-01,\n",
      "        -2.0838e-01, -3.1463e-01,  1.4264e-01, -3.1047e-01, -2.9660e-01,\n",
      "        -3.9048e-01,  2.6769e-01,  1.8550e-01,  3.5339e+00, -3.7971e-01,\n",
      "        -3.6620e-01, -3.9953e-01, -3.1660e-01,  8.0689e-02, -4.1876e-01,\n",
      "        -4.7962e+00,  1.4821e-01, -1.2325e-01,  4.5694e-01, -6.2773e-01,\n",
      "         7.8319e-01, -4.9245e-01, -9.1689e-01, -1.8093e-01,  5.4773e-01,\n",
      "         6.7262e-01, -1.2675e-01, -2.6542e-01, -6.1293e-02,  1.0408e-01,\n",
      "         1.4179e+00,  6.1188e-01, -1.8941e-01,  3.6685e-01, -4.8201e-02,\n",
      "        -5.7905e-01, -3.0301e-01,  8.8375e-01]), 'label': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_825487/2923062832.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = torch.load(\"dev_data.pt\")[\"data\"]\n"
     ]
    }
   ],
   "source": [
    "a = torch.load(\"dev_data.pt\")[\"data\"]\n",
    "print(a[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
